{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supported-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import scipy\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import datetime \n",
    "import sklearn.linear_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-bridges",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arctic-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGE_TASK_ID = 200\n",
    "#\n",
    "date_min = \"20200601\"\n",
    "date_max = \"20201231\"\n",
    "#\n",
    "SIC_trend_period = 5\n",
    "lead_time_max = 10\n",
    "#\n",
    "paths = {}\n",
    "paths[\"output\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Nearest_neighbor/\"\n",
    "paths[\"LSM\"] = \"/lustre/storeB/project/copernicus/svalnav/Data/TOPAZ4/\"\n",
    "paths[\"TOPAZ\"] = \"/lustre/storeB/project/copernicus/svalnav/Data_operational_ice_drift_forecasts/Pan_Arctic/TOPAZ4/\"\n",
    "paths[\"ECMWF\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/ECMWF/\"\n",
    "paths[\"OSISAF\"] = \"/lustre/storeB/project/copernicus/osisaf/data/reprocessed/ice/conc-cont-reproc/v3p0/\"\n",
    "paths[\"AMSR2\"] = \"/lustre/storeB/project/copernicus/osisaf/data/reprocessed/ice/conc-cont-reproc/v3p0/\"\n",
    "#\n",
    "proj = {}\n",
    "proj[\"TOPAZ\"] = \"+proj=stere +lon_0=-45. +lat_ts=90. +lat_0=90. +a=6378273. +b=6378273. +ellps=sphere\"\n",
    "proj[\"ECMWF\"] = \"+proj=latlon\"\n",
    "proj[\"OSISAF\"] = \"+proj=laea +lon_0=0 +datum=WGS84 +ellps=WGS84 +lat_0=90.0\"\n",
    "proj[\"AMSR2\"] = \"+proj=laea +lon_0=0 +datum=WGS84 +ellps=WGS84 +lat_0=90.0\"\n",
    "#\n",
    "crs = {}\n",
    "for var in proj:\n",
    "    crs[var] = pyproj.CRS.from_proj4(proj[var])\n",
    "#\n",
    "variables = {}\n",
    "variables[\"LSM\"] = [\"LSM\"]\n",
    "variables[\"TOPAZ\"] = [\"fice\", \"hice\"]\n",
    "variables[\"ECMWF\"] = [\"U10M\", \"V10M\", \"T2M\"]\n",
    "variables[\"OSISAF\"] = [\"ice_conc\", \"trend\"]  # trend is calculated from the last n days before the forecast start date\n",
    "variables[\"AMSR2\"] = [\"ice_conc\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-former",
   "metadata": {},
   "source": [
    "task_date function\n",
    "\n",
    "    date_min: earliest forecast start date to process\n",
    "    date_max: latest forecast start date to process\n",
    "    task_ID: task ID when parallelizing (SGE_TASK_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "studied-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_date(date_min, date_max, task_ID):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    list_date = []\n",
    "    while current_date <= end_date:\n",
    "        list_date.append(current_date.strftime('%Y%m%d'))\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    date_task = list_date[task_ID - 1]\n",
    "    return(date_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-founder",
   "metadata": {},
   "source": [
    "ECMWF_time_steps_to_daily_time_steps function => Compute the daily mean of the variable for each days  \n",
    "    \n",
    "    time_ECMWF: time variable in ECMWF netCDF files\n",
    "    field: 2D array variable\n",
    "    ndays: number of days (lead time) to compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acknowledged-blast",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ECMWF_time_steps_to_daily_time_steps(time_ECMWF, field, ndays):\n",
    "    lead_time = time_ECMWF - time_ECMWF[0]\n",
    "    ts_start = np.linspace(0 * 24, (ndays - 1) * 24, ndays)\n",
    "    ts_end = ts_start + 24\n",
    "    daily_field = np.full((ndays, field.shape[1], field.shape[2]), np.nan)\n",
    "    #\n",
    "    for ts in range(0, ndays):\n",
    "        lead_time_idx = np.squeeze(np.where(np.logical_and(lead_time >= ts_start[ts], lead_time < ts_end[ts])))\n",
    "        if ts == 3:\n",
    "            daily_field[ts,:,:] = (18 * np.nanmean(np.ma.squeeze(field[lead_time_idx[0:18],:,:]), axis = 0) \\\n",
    "                                  + 6 * np.nanmean(np.ma.squeeze(field[lead_time_idx[18:20],:,:]), axis = 0)) / 24\n",
    "        else:\n",
    "            daily_field[ts,:,:] = np.nanmean(np.ma.squeeze(field[lead_time_idx,:,:]), axis = 0)\n",
    "    #\n",
    "    return(daily_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-causing",
   "metadata": {
    "tags": []
   },
   "source": [
    "Padding function (make_padding)  \n",
    "\n",
    "    x and y must be vectors (can be latitude / longitude if the data are on a regular grid)  \n",
    "    field must be either a 2D array (y, x) or a 3D array (time, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cloudy-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding(x, y, field):\n",
    "    dx = x[1] - x[0]\n",
    "    x_extent = np.pad(x, (1, 1), constant_values = np.nan)    \n",
    "    x_extent[0] = x_extent[1] - dx\n",
    "    x_extent[-1] = x_extent[-2] + dx\n",
    "    #\n",
    "    dy = y[1] - y[0]\n",
    "    y_extent = np.pad(y, (1, 1), constant_values = np.nan)\n",
    "    y_extent[0] = y_extent[1] - dy\n",
    "    y_extent[-1] = y_extent[-2] + dy\n",
    "    #\n",
    "    if field.ndim == 2:\n",
    "        field_extent = np.pad(field, (1,1), constant_values = np.nan)\n",
    "    elif field.ndim == 3:\n",
    "        time_dim = len(field[:,0,0])\n",
    "        field_extent = np.full((time_dim, len(y_extent), len(x_extent)), np.nan)\n",
    "        #\n",
    "        for t in range(0, time_dim):\n",
    "            field_extent[t,:,:] = np.pad(field[t,:,:], (1,1), constant_values = np.nan)\n",
    "    #\n",
    "    return(x_extent, y_extent, field_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-drink",
   "metadata": {},
   "source": [
    "Regridding functions (nearest_neighbor_indexes and nearest_neighbor_interp)  \n",
    "\n",
    "    xx_input and yy_input must be 2D arrays\n",
    "    x_output and y_output must be vectors  \n",
    "    field must be either a 2D array with dimensions (y, x) or a 3D array with dimensions (time, y, x) \n",
    "    invalid_values = fill value to remove from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seasonal-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_indexes(x_input, y_input, x_output, y_output):\n",
    "    x_input = np.expand_dims(x_input, axis = 1)\n",
    "    y_input = np.expand_dims(y_input, axis = 1)\n",
    "    x_output = np.expand_dims(x_output, axis = 1)\n",
    "    y_output = np.expand_dims(y_output, axis = 1)\n",
    "    #\n",
    "    coord_input = np.concatenate((x_input, y_input), axis = 1)\n",
    "    coord_output = np.concatenate((x_output, y_output), axis = 1)\n",
    "    #\n",
    "    tree = scipy.spatial.KDTree(coord_input)\n",
    "    dist, idx = tree.query(coord_output)\n",
    "    #\n",
    "    return(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_interp(xx_input, yy_input, x_output, y_output, field, fill_value = None):\n",
    "    xx_input_flat = np.ndarray.flatten(xx_input)\n",
    "    yy_input_flat = np.ndarray.flatten(yy_input)\n",
    "    #\n",
    "    if fill_value is not None:\n",
    "        if field.ndim == 2:\n",
    "            idx_fill_value = np.ndarray.flatten(field) == fill_value\n",
    "        elif field.ndim == 3:\n",
    "            idx_fill_value = np.ndarray.flatten(field[0,:,:]) == fill_value\n",
    "        #\n",
    "        xx_input_flat = xx_input_flat[idx_fill_value == False]\n",
    "        yy_input_flat = yy_input_flat[idx_fill_value == False]\n",
    "    #\n",
    "    xx_output, yy_output = np.meshgrid(x_output, y_output)\n",
    "    xx_output_flat = np.ndarray.flatten(xx_output)\n",
    "    yy_output_flat = np.ndarray.flatten(yy_output)\n",
    "    #\n",
    "    idx = nearest_neighbor_indexes(xx_input_flat, yy_input_flat, xx_output_flat, yy_output_flat)\n",
    "    #\n",
    "    if field.ndim == 2:\n",
    "        field_flat = np.ndarray.flatten(field)\n",
    "        if fill_value is not None:\n",
    "            field_flat = field_flat[idx_fill_value == False]\n",
    "        #\n",
    "        field_interp = field_flat[idx]\n",
    "        field_regrid = np.reshape(field_interp, (len(y_output), len(x_output)), order = \"C\")\n",
    "    #    \n",
    "    elif field.ndim == 3:\n",
    "        time_dim = len(field[:,0,0])\n",
    "        field_regrid = np.full((time_dim, len(y_output), len(x_output)), np.nan)\n",
    "        #\n",
    "        for t in range(0, time_dim):\n",
    "            field_flat = np.ndarray.flatten(field[t,:,:])\n",
    "            if fill_value is not None:\n",
    "                field_flat = field_flat[idx_fill_value == False]\n",
    "            #\n",
    "            field_interp = field_flat[idx]\n",
    "            field_regrid[t,:,:] = np.reshape(field_interp, (len(y_output), len(x_output)), order = \"C\")\n",
    "    #\n",
    "    return(field_regrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-supplier",
   "metadata": {},
   "source": [
    "rotate_wind function\n",
    "\n",
    "    x_wind, y_wind, lats, lons must be numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "biological-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_wind(x_wind, y_wind, lats, lons, proj_str_from, proj_str_to):\n",
    "    if np.shape(x_wind) != np.shape(y_wind):\n",
    "        raise ValueError(f\"x_wind {np.shape(x_wind)} and y_wind {np.shape(y_wind)} arrays must be the same size\")\n",
    "    if len(lats.shape) != 1:\n",
    "        raise ValueError(f\"lats {np.shape(lats)} must be 1D\")\n",
    "    if np.shape(lats) != np.shape(lons):\n",
    "        raise ValueError(f\"lats {np.shape(lats)} and lats {np.shape(lons)} must be the same size\")\n",
    "    if len(np.shape(x_wind)) == 1:\n",
    "        if np.shape(x_wind) != np.shape(lats):\n",
    "            raise ValueError(f\"x_wind {len(x_wind)} and lats {len(lats)} arrays must be the same size\")\n",
    "    elif len(np.shape(x_wind)) == 2:\n",
    "        if x_wind.shape[1] != len(lats):\n",
    "            raise ValueError(f\"Second dimension of x_wind {x_wind.shape[1]} must equal number of lats {len(lats)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"x_wind {np.shape(x_wind)} must be 1D or 2D\")\n",
    "    #\n",
    "    proj_from = pyproj.Proj(proj_str_from)\n",
    "    proj_to = pyproj.Proj(proj_str_to)\n",
    "    transformer = pyproj.transformer.Transformer.from_proj(proj_from, proj_to)\n",
    "    #\n",
    "    orig_speed = np.sqrt(x_wind**2 + y_wind**2)\n",
    "    #\n",
    "    x0, y0 = proj_from(lons, lats)\n",
    "    if proj_from.name != \"longlat\":\n",
    "        x1 = x0 + x_wind\n",
    "        y1 = y0 + y_wind\n",
    "    else:\n",
    "        factor = 3600000.0\n",
    "        x1 = x0 + x_wind / factor / np.cos(lats * 3.14159265 / 180)\n",
    "        y1 = y0 + y_wind / factor\n",
    "    #\n",
    "    X0, Y0 = transformer.transform(x0, y0)\n",
    "    X1, Y1 = transformer.transform(x1, y1)\n",
    "    #\n",
    "    new_x_wind = X1 - X0\n",
    "    new_y_wind = Y1 - Y0\n",
    "    #\n",
    "    if proj_to.name == \"longlat\":\n",
    "        new_x_wind *= np.cos(lats * 3.14159265 / 180)\n",
    "    #\n",
    "    if proj_to.name == \"longlat\" or proj_from.name == \"longlat\":\n",
    "        curr_speed = np.sqrt(new_x_wind**2 + new_y_wind**2)\n",
    "        new_x_wind *= orig_speed / curr_speed\n",
    "        new_y_wind *= orig_speed / curr_speed\n",
    "    #\n",
    "    return(new_x_wind, new_y_wind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-murray",
   "metadata": {},
   "source": [
    "read_netCDF functions  \n",
    "\n",
    "    filename: filename including the path\n",
    "    variables: list of variables (excluding time, x, y, lat, lon) to extract (list of strings)\n",
    "    paths: dictionary defined in the Constants section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "affecting-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_netCDF(filename, variables, paths = paths):\n",
    "    Dataset = {}\n",
    "    nc = netCDF4.Dataset(filename, \"r\")\n",
    "    #\n",
    "    if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "        xmin = 65\n",
    "        xmax = 609\n",
    "        ymin = 70\n",
    "        ymax = 614\n",
    "    #\n",
    "    if (paths[\"LSM\"] in filename) == False:\n",
    "        Dataset[\"time\"] = nc.variables[\"time\"][:]\n",
    "    #\n",
    "    if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "        Dataset[\"x\"] = nc.variables[\"x\"][xmin:xmax] * 100 * 1000\n",
    "        Dataset[\"y\"] = nc.variables[\"y\"][ymin:ymax] * 100 * 1000\n",
    "        Dataset[\"lat\"] = nc.variables[\"latitude\"][ymin:ymax, xmin:xmax] \n",
    "        Dataset[\"lon\"] = nc.variables[\"longitude\"][ymin:ymax, xmin:xmax]\n",
    "    #\n",
    "    elif paths[\"ECMWF\"] in filename:\n",
    "        Dataset[\"lat\"] = nc.variables[\"lat\"][:]\n",
    "        Dataset[\"lon\"] = nc.variables[\"lon\"][:]\n",
    "    #\n",
    "    elif paths[\"OSISAF\"] in filename:\n",
    "        Dataset[\"x\"] = nc.variables[\"xc\"][:] * 1000\n",
    "        Dataset[\"y\"] = nc.variables[\"yc\"][:] * 1000\n",
    "        Dataset[\"lat\"] = nc.variables[\"lat\"][:,:] \n",
    "        Dataset[\"lon\"] = nc.variables[\"lon\"][:,:]\n",
    "    #\n",
    "    for var in variables:\n",
    "        vardim = nc.variables[var].ndim\n",
    "        if vardim == 1:\n",
    "            Dataset[var] = nc.variables[var][:]\n",
    "        elif vardim == 2:\n",
    "            if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "                Dataset[var] = nc.variables[var][ymin:ymax, xmin:xmax]\n",
    "            else:\n",
    "                Dataset[var] = nc.variables[var][:,:]\n",
    "        elif vardim == 3:\n",
    "            if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "                if var == \"fice\":\n",
    "                    Dataset[var] = nc.variables[var][:, ymin:ymax, xmin:xmax] * 100\n",
    "                else:\n",
    "                    Dataset[var] = nc.variables[var][:, ymin:ymax, xmin:xmax]\n",
    "            else:\n",
    "                Dataset[var] = nc.variables[var][:,:,:]\n",
    "        else:\n",
    "            print(\"ERROR. Number of dimensions higher than 3.\")\n",
    "    nc.close()\n",
    "    #\n",
    "    return(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-manual",
   "metadata": {},
   "source": [
    "extract_TOPAZ_data function\n",
    "\n",
    "    date_task: forecast start date of the TOPAZ forecasts\n",
    "    ndays: maximum lead time in days\n",
    "    variables: list of variables to extract (variables[\"TOPAZ\"]) \n",
    "    paths = path where the TOPAZ data are stored (paths[\"TOPAZ\"]). Defined in the Constant section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wanted-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_TOPAZ_data(date_task, ndays, variables = variables[\"TOPAZ\"], paths = paths):\n",
    "    Data_output = {}\n",
    "    filename_TOPAZ = paths[\"TOPAZ\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\" + \"TOPAZ4_daily_\" + date_task + \".nc\"\n",
    "    TOPAZ = read_netCDF(filename_TOPAZ, variables)\n",
    "    #\n",
    "    for var in [\"time\", \"x\", \"y\", \"lat\", \"lon\"]:\n",
    "        Data_output[var] = TOPAZ[var]\n",
    "    #\n",
    "    xx, yy = np.meshgrid(Data_output[\"x\"], Data_output[\"y\"])\n",
    "    xx_flat = np.ndarray.flatten(xx)\n",
    "    yy_flat = np.ndarray.flatten(yy)\n",
    "    #\n",
    "    for var in variables:\n",
    "        TOPAZ[var][np.isnan(TOPAZ[var]) == True] = -32767\n",
    "        Data_output[var] = nearest_neighbor_interp(xx_flat, yy_flat, Data_output[\"x\"], Data_output[\"y\"], TOPAZ[var], fill_value = -32767)\n",
    "    #\n",
    "    return(Data_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-invention",
   "metadata": {},
   "source": [
    "extract_ECMWF_data function   \n",
    "\n",
    "    filename: filename (including path) containing ECMWF data \n",
    "    ndays: maximum lead time in days\n",
    "    TOPAZ: TOPAZ dataset (dictionary)   \n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract (variables[\"ECMWF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "missing-mississippi",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_ECMWF_data(filename, ndays, TOPAZ, proj = proj, variables = variables[\"ECMWF\"], crs = crs):\n",
    "    ECMWF = read_netCDF(filename, variables)\n",
    "    Data_TOPAZgrid = {}\n",
    "    transform_ECMWF_to_TOPAZ = pyproj.Transformer.from_crs(crs[\"ECMWF\"], crs[\"TOPAZ\"], always_xy = True)\n",
    "    lons, lats = np.meshgrid(ECMWF[\"lon\"], ECMWF[\"lat\"])\n",
    "    xx_ECMWF_TOPAZproj, yy_ECMWF_TOPAZproj = transform_ECMWF_to_TOPAZ.transform(lons, lats)\n",
    "    #\n",
    "    Data_ECMWFgrid = {}\n",
    "    for var in variables:\n",
    "        Data_ECMWFgrid[var] = ECMWF_time_steps_to_daily_time_steps(ECMWF[\"time\"], ECMWF[var], ndays)\n",
    "    #\n",
    "    if (\"U10M\" in Data_ECMWFgrid) and (\"V10M\" in Data_ECMWFgrid):\n",
    "        x_wind = np.full((len(TOPAZ[\"time\"]), len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        y_wind = np.full((len(TOPAZ[\"time\"]), len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        #\n",
    "        for ts in range(0, len(TOPAZ[\"time\"])):\n",
    "            x_wind_rot, y_wind_rot = rotate_wind(np.ndarray.flatten(Data_ECMWFgrid[\"U10M\"][ts,:,:]), \n",
    "                                                 np.ndarray.flatten(Data_ECMWFgrid[\"V10M\"][ts,:,:]),\n",
    "                                                 np.ndarray.flatten(lats), \n",
    "                                                 np.ndarray.flatten(lons), \n",
    "                                                 proj[\"ECMWF\"], \n",
    "                                                 proj[\"TOPAZ\"]\n",
    "                                                )\n",
    "            #\n",
    "            x_wind[ts,:,:] = np.reshape(x_wind_rot, (len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), order = \"C\")\n",
    "            y_wind[ts,:,:] = np.reshape(y_wind_rot, (len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), order = \"C\")            \n",
    "            #\n",
    "        Data_ECMWFgrid[\"wind_x\"] = np.copy(x_wind)\n",
    "        Data_ECMWFgrid[\"wind_y\"] = np.copy(y_wind)\n",
    "        Data_ECMWFgrid.pop(\"U10M\")\n",
    "        Data_ECMWFgrid.pop(\"V10M\")\n",
    "    #\n",
    "    Cum_data_ECMWFgrid = {}\n",
    "    for var in Data_ECMWFgrid:\n",
    "        var_cum = np.full((len(TOPAZ[\"time\"]), len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        for ts in range(0, len(TOPAZ[\"time\"])):\n",
    "            var_cum[ts,:,:] = np.nanmean(Data_ECMWFgrid[var][0:ts+1,:,:], axis = 0)\n",
    "        #\n",
    "        Cum_data_ECMWFgrid[var + \"_cum\"] = np.copy(var_cum)\n",
    "        Cum_data_ECMWFgrid[var + \"_cum\"][np.isnan(var_cum) == True] = -32767\n",
    "    #\n",
    "    for var in Cum_data_ECMWFgrid:\n",
    "        Data_TOPAZgrid[var] = nearest_neighbor_interp(xx_ECMWF_TOPAZproj, yy_ECMWF_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], Cum_data_ECMWFgrid[var], fill_value = -32767)\n",
    "    #\n",
    "    return(Data_TOPAZgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-porcelain",
   "metadata": {},
   "source": [
    "SIC_trend function\n",
    "\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    trend_period: Number of days to take into account for calculating the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "monetary-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIC_trend(date_task, trend_period):\n",
    "    Dataset = {}\n",
    "    first_date = datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = trend_period)\n",
    "    last_date = datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = 1)\n",
    "    current_date = first_date\n",
    "    time_vect = []\n",
    "    #\n",
    "    for id in range(0, trend_period):\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        if int(date_task[0:4]) < 2021:\n",
    "            filename_SIC = paths[\"OSISAF\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_cdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "        else:\n",
    "            filename_SIC = paths[\"OSISAF\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_icdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "        #\n",
    "        if os.path.isfile(filename_SIC):\n",
    "            time_vect.append(id)\n",
    "            SIC_data = read_netCDF(filename_SIC, variables = [\"ice_conc\"])\n",
    "            #\n",
    "            if \"SIC_all\" in locals():\n",
    "                SIC_all = np.concatenate((SIC_all, SIC_data[\"ice_conc\"]), axis = 0)\n",
    "            else:\n",
    "                SIC_all = np.copy(SIC_data[\"ice_conc\"])\n",
    "            #\n",
    "            if current_date == last_date:\n",
    "                for var in SIC_data:\n",
    "                    Dataset[var] = np.copy(SIC_data[var])\n",
    "            #\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    #\n",
    "    if len(time_vect) >= 3:\n",
    "        trend = np.zeros((SIC_all.shape[1], SIC_all.shape[2]))\n",
    "        x = np.array(time_vect).reshape((-1,1))\n",
    "        for i in range(0, SIC_all.shape[1]):\n",
    "            for j in range(0, SIC_all.shape[2]):\n",
    "                y = SIC_all[:,i,j]\n",
    "                if np.all(y >= 0):\n",
    "                    model = sklearn.linear_model.LinearRegression().fit(x, y)\n",
    "                    trend[i,j] = model.coef_[0]\n",
    "        Dataset[\"trend\"] = np.copy(trend)\n",
    "    #\n",
    "    return(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-andrew",
   "metadata": {},
   "source": [
    "extract_SIC_obs_predictors function\n",
    "    \n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    trend_period: Number of days to take into account for calculating the trend\n",
    "    TOPAZ: TOPAZ dataset (dictionary)   \n",
    "    LSM: TOPAZ land sea mask (dictionary)\n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract (variables[\"OSISAF\"])\n",
    "    crs: crs defined in \"Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "american-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_SIC_obs_predictors(date_task, trend_period, TOPAZ, LSM, proj = proj, variables = variables[\"OSISAF\"], crs = crs):\n",
    "    Data_TOPAZgrid = {}\n",
    "    SIC_obs = SIC_trend(date_task, trend_period)\n",
    "    #\n",
    "    transform_TOPAZ_to_SIC = pyproj.Transformer.from_crs(crs[\"TOPAZ\"], crs[\"OSISAF\"], always_xy = True)\n",
    "    x_LSM_pad, y_LSM_pad, LSM_pad = make_padding(LSM[\"x\"], LSM[\"y\"], LSM[\"LSM\"])\n",
    "    LSM_pad[np.isnan(LSM_pad) == True] = 0  # Assigned to land grid point\n",
    "    xx_LSM, yy_LSM = np.meshgrid(x_LSM_pad, y_LSM_pad)\n",
    "    xx_LSM_SICproj, yy_LSM_SICproj = transform_TOPAZ_to_SIC.transform(xx_LSM, yy_LSM)\n",
    "    LSM_SICproj = nearest_neighbor_interp(xx_LSM_SICproj, yy_LSM_SICproj, SIC_obs[\"x\"], SIC_obs[\"y\"], LSM_pad, fill_value = None)\n",
    "    #\n",
    "    transform_SIC_to_TOPAZ = pyproj.Transformer.from_crs(crs[\"OSISAF\"], crs[\"TOPAZ\"], always_xy = True)\n",
    "    xx_SIC_obs, yy_SIC_obs = np.meshgrid(SIC_obs[\"x\"], SIC_obs[\"y\"])\n",
    "    xx_SIC_TOPAZproj, yy_SIC_TOPAZproj = transform_SIC_to_TOPAZ.transform(xx_SIC_obs, yy_SIC_obs)\n",
    "    #\n",
    "    for var in variables:\n",
    "        SIC_obs[var] = np.squeeze(SIC_obs[var])\n",
    "        SIC_obs[var][LSM_SICproj == 0] = -32767\n",
    "        Data_TOPAZgrid[var] = nearest_neighbor_interp(xx_SIC_TOPAZproj, yy_SIC_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], SIC_obs[var], fill_value = -32767)\n",
    "    #\n",
    "    return(Data_TOPAZgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-serum",
   "metadata": {},
   "source": [
    "extract_targets (MUST BE UPDATED WITH AMSR2 SPECIFICATIONS)\n",
    "\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    ndays: Number of days to take into account for calculating the trend\n",
    "    TOPAZ: TOPAZ dataset (dictionary)\n",
    "    LSM: TOPAZ land sea maskLSM: TOPAZ land sea mask\n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract (variables[\"OSISAF\"])\n",
    "    crs: crs defined in \"Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "south-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(date_task, ndays, TOPAZ, LSM, proj = proj, crs = crs, paths = paths):\n",
    "    Data_TOPAZgrid = {}\n",
    "    #\n",
    "    transform_TOPAZ_to_SIC = pyproj.Transformer.from_crs(crs[\"TOPAZ\"], crs[\"AMSR2\"], always_xy = True)\n",
    "    x_LSM_pad, y_LSM_pad, LSM_pad = make_padding(LSM[\"x\"], LSM[\"y\"], LSM[\"LSM\"])\n",
    "    LSM_pad[np.isnan(LSM_pad) == True] = 0  # Assigned land grid point to NaN\n",
    "    xx_LSM, yy_LSM = np.meshgrid(x_LSM_pad, y_LSM_pad)\n",
    "    xx_LSM_SICproj, yy_LSM_SICproj = transform_TOPAZ_to_SIC.transform(xx_LSM, yy_LSM)\n",
    "    #\n",
    "    transform_SIC_to_TOPAZ = pyproj.Transformer.from_crs(crs[\"AMSR2\"], crs[\"TOPAZ\"], always_xy = True)\n",
    "    #\n",
    "    for lt in range(0, ndays):\n",
    "        date_str = (datetime.datetime.strptime(date_task, \"%Y%m%d\") + datetime.timedelta(days = lt)).strftime(\"%Y%m%d\")\n",
    "        if int(date_task[0:4]) < 2021:\n",
    "            filename_SIC = paths[\"AMSR2\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_cdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "        else:\n",
    "            filename_SIC = paths[\"AMSR2\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_icdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "        SIC_obs = read_netCDF(filename_SIC, variables = [\"ice_conc\"])\n",
    "        #\n",
    "        if lt == 0:\n",
    "            LSM_SICproj = nearest_neighbor_interp(xx_LSM_SICproj, yy_LSM_SICproj, SIC_obs[\"x\"], SIC_obs[\"y\"], LSM_pad, fill_value = None)\n",
    "            LSM_SICproj = np.expand_dims(LSM_SICproj, axis = 0)\n",
    "            xx_SIC_obs, yy_SIC_obs = np.meshgrid(SIC_obs[\"x\"], SIC_obs[\"y\"])\n",
    "            xx_SIC_TOPAZproj, yy_SIC_TOPAZproj = transform_SIC_to_TOPAZ.transform(xx_SIC_obs, yy_SIC_obs)\n",
    "        #\n",
    "        SIC_obs[\"ice_conc\"][LSM_SICproj == 0] = -32767\n",
    "        SIC_TOPAZgrid = nearest_neighbor_interp(xx_SIC_TOPAZproj, yy_SIC_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], SIC_obs[\"ice_conc\"], fill_value = -32767)\n",
    "        #\n",
    "        if lt == 0:\n",
    "            Data_TOPAZgrid[\"SIC\"] = np.copy(SIC_TOPAZgrid)\n",
    "        else:\n",
    "            Data_TOPAZgrid[\"SIC\"] = np.concatenate((Data_TOPAZgrid[\"SIC\"], SIC_TOPAZgrid), axis = 0)\n",
    "    #\n",
    "    Data_TOPAZgrid[\"SIE_10\"] = np.zeros(np.shape(Data_TOPAZgrid[\"SIC\"]))\n",
    "    Data_TOPAZgrid[\"SIE_20\"] = np.zeros(np.shape(Data_TOPAZgrid[\"SIC\"]))\n",
    "    Data_TOPAZgrid[\"SIE_10\"][Data_TOPAZgrid[\"SIC\"] >= 10] = 1\n",
    "    Data_TOPAZgrid[\"SIE_20\"][Data_TOPAZgrid[\"SIC\"] >= 20] = 1\n",
    "    #\n",
    "    Data_TOPAZgrid[\"TOPAZ_error\"] = TOPAZ[\"fice\"] - Data_TOPAZgrid[\"SIC\"]\n",
    "    #\n",
    "    return(Data_TOPAZgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-alexandria",
   "metadata": {},
   "source": [
    "write_netCDF function\n",
    "\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    Datasets: Dictionary containing all variables that we want to extract\n",
    "    paths: paths defined in the Constants section\n",
    "    trend_period: Number of days to take into account for calculating the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prompt-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_netCDF(date_task, Datasets, paths, trend_period):\n",
    "    Outputs = vars()\n",
    "    #\n",
    "    path_output = paths[\"output\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\"\n",
    "    if os.path.exists(path_output) == False:\n",
    "        os.system(\"mkdir -p \" + path_output)    \n",
    "    output_filename = path_output + \"Dataset_\" + date_task + \".nc\"\n",
    "    if os.path.isfile(output_filename):\n",
    "        os.system(\"rm \" + output_filename)\n",
    "    output_netcdf = netCDF4.Dataset(output_filename, 'w', format = 'NETCDF4')\n",
    "    #\n",
    "    dimensions = [\"time\", \"x\", \"y\"]\n",
    "    for di in dimensions:\n",
    "        Outputs[di] = output_netcdf.createDimension(di, len(Datasets[\"TOPAZ\"][di]))\n",
    "    #\n",
    "    dim_variables = dimensions + [\"lat\", \"lon\"]\n",
    "    for dv in dim_variables:\n",
    "        if Datasets[\"TOPAZ\"][dv].ndim == 1:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (dv))\n",
    "            Outputs[dv][:] = Datasets[\"TOPAZ\"][dv]\n",
    "            if dv == \"time\":\n",
    "                Outputs[dv].standard_name = \"forecast time\"\n",
    "                Outputs[dv].units = \"hours since 1950-1-1T00:00:00Z\"\n",
    "            elif dv == \"x\" or dv == \"y\":\n",
    "                Outputs[dv].standard_name = \"projection_\" + dv + \"_coordinate\"\n",
    "                Outputs[dv].units = \"m\"\n",
    "        elif Datasets[\"TOPAZ\"][dv].ndim == 2:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (\"y\", \"x\"))\n",
    "            Outputs[dv][:,:] = Datasets[\"TOPAZ\"][dv]\n",
    "            if dv == \"lat\":\n",
    "                Outputs[dv].standard_name = \"latitude\"\n",
    "            elif dv == \"lon\":\n",
    "                Outputs[dv].standard_name = \"longitude\"\n",
    "            Outputs[dv].units = \"degrees\"\n",
    "    #\n",
    "    SIC_variables = [\"ice_conc\", \"fice\", \"SIC\"]\n",
    "    for ds in Datasets:\n",
    "        for var in Datasets[ds]:\n",
    "            if (var in dim_variables) == False:\n",
    "                if var == \"LSM\":\n",
    "                    var_name = \"LSM\"\n",
    "                elif var in SIC_variables:\n",
    "                    var_name = ds + \"_SIC\"\n",
    "                else:\n",
    "                    var_name = ds + \"_\" + var\n",
    "                #\n",
    "                if Datasets[ds][var].ndim == 2:\n",
    "                    Outputs[var_name] = output_netcdf.createVariable(var_name, \"d\", (\"y\", \"x\"))\n",
    "                    Outputs[var_name][:,:] = np.round(Datasets[ds][var], 3)\n",
    "                elif Datasets[ds][var].ndim == 3:\n",
    "                    Outputs[var_name] = output_netcdf.createVariable(var_name, \"d\", (\"time\", \"y\", \"x\"))\n",
    "                    Outputs[var_name][:,:,:] = np.round(Datasets[ds][var], 3)\n",
    "                #\n",
    "                if var in SIC_variables:\n",
    "                    if ds == \"TARGET\":\n",
    "                        Outputs[var_name].standard_name = \"AMSR2 sea ice concentration\"\n",
    "                    else: \n",
    "                        Outputs[var_name].standard_name = ds + \" sea ice concentration\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                if \"SIE_\" in var:\n",
    "                    Outputs[var_name].standard_name = \"Sea ice extent with a sea ice concentration higher than \" + var[-2:len(var)] + \" %\"\n",
    "                    Outputs[var_name].units = \"1 if sea ice concentration higher than \" + var[-2:len(var)] + \" %, 0 otherwise\"\n",
    "                elif var == \"trend\":\n",
    "                    Outputs[var_name].standard_name = \"Sea ice concentration trend over the \" + str(SIC_trend_period) + \" days preceding the forecast start date\"\n",
    "                    Outputs[var_name].units = \"% / day\"\n",
    "                elif var == \"TOPAZ_error\":\n",
    "                    Outputs[var_name].standard_name = \"TOPAZ error in sea ice concentration (TOPAZ - AMSR2 observations)\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                elif ds + \"_\" + var == \"TOPAZ_hice\":\n",
    "                    Outputs[var_name].standard_name = \"TOPAZ sea ice thickness\"\n",
    "                    Outputs[var_name].units = \"m\"\n",
    "                elif var == \"LSM\":\n",
    "                    Outputs[var_name].standard_name = \"Land sea mask\"\n",
    "                    Outputs[var_name].units = \"1: ocean, 0: land\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_T2M\":\n",
    "                    Outputs[var_name].standard_name = \"ECMWF 2 metre temperature\"\n",
    "                    Outputs[var_name].units = \"K\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_wind_x_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean ECMWF wind in the x direction since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"m/s\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_wind_y_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean ECMWF wind in the y direction since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"m/s\"\n",
    "    output_netcdf.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-booth",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "liked-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_task 20201217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/300610617.py:30: RuntimeWarning: Mean of empty slice\n",
      "  x_wind_cum[ts,:,:] = np.nanmean(x_wind[0:ts+1,:,:], axis = 0)\n",
      "/tmp/ipykernel_1064325/300610617.py:31: RuntimeWarning: Mean of empty slice\n",
      "  y_wind_cum[ts,:,:] = np.nanmean(y_wind[0:ts+1,:,:], axis = 0)\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1064325/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#\n",
    "date_task = task_date(date_min, date_max, task_ID = SGE_TASK_ID)\n",
    "print(\"date_task\", date_task)\n",
    "#\n",
    "filename_LSM = paths[\"LSM\"] + \"TOPAZ4_land_sea_mask.nc\"\n",
    "filename_ECMWF = paths[\"ECMWF\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/ECMWF_operational_forecasts_T2m_10mwind_\" + date_task + \"_NH.nc\"\n",
    "#\n",
    "Datasets = {}\n",
    "#\n",
    "Datasets[\"LSM\"] = read_netCDF(filename = filename_LSM, \n",
    "                              variables = variables[\"LSM\"])   # 1 ocean / 0 land\n",
    "#\n",
    "Datasets[\"TOPAZ\"] = extract_TOPAZ_data(date_task = date_task, \n",
    "                                       ndays = lead_time_max, \n",
    "                                       variables = variables[\"TOPAZ\"], \n",
    "                                       paths = paths)\n",
    "#\n",
    "Datasets[\"ECMWF\"] = extract_ECMWF_data(filename = filename_ECMWF, \n",
    "                                       ndays = lead_time_max, \n",
    "                                       TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                       variables = variables[\"ECMWF\"], \n",
    "                                       crs = crs)\n",
    "#\n",
    "Datasets[\"SICobs\"] = extract_SIC_obs_predictors(date_task = date_task, \n",
    "                                                trend_period = SIC_trend_period, \n",
    "                                                TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                                LSM = Datasets[\"LSM\"], \n",
    "                                                proj = proj, variables = variables[\"OSISAF\"], \n",
    "                                                crs = crs)\n",
    "#\n",
    "Datasets[\"TARGET\"] = extract_targets(date_task = date_task, \n",
    "                                     ndays = lead_time_max, \n",
    "                                     TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                     LSM = Datasets[\"LSM\"], \n",
    "                                     proj = proj, \n",
    "                                     crs = crs, \n",
    "                                     paths = paths)\n",
    "#\n",
    "write_netCDF(date_task = date_task, \n",
    "             Datasets = Datasets, \n",
    "             paths = paths, \n",
    "             trend_period = SIC_trend_period)\n",
    "#\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cleared-horizon",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_all 73.98313093185425\n"
     ]
    }
   ],
   "source": [
    "dt_all = t1 - t0\n",
    "#\n",
    "print(\"dt_all\", dt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48692858-5349-4523-ba8d-319d2a8a3a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:production-10-2022] *",
   "language": "python",
   "name": "conda-env-production-10-2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
