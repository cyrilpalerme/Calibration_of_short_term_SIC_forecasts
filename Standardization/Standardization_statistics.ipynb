{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dcab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import netCDF4\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d88ba8",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4b41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min = \"20130103\"\n",
    "date_max = \"20201231\"\n",
    "#\n",
    "paths = {}\n",
    "paths[\"data\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Land_free_ocean/\"\n",
    "paths[\"output\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Standardization/\"\n",
    "#\n",
    "list_variables = {}\n",
    "list_variables[\"data\"] = []\n",
    "list_variables[\"geolocation\"] = [\"time\", \"x\", \"y\", \"lat\", \"lon\"]\n",
    "#\n",
    "frequency = \"weekly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38ac92",
   "metadata": {},
   "source": [
    "extract_dataset function\n",
    "\n",
    "    date_min: earliest date to consider\n",
    "    date_max: latest date to consider\n",
    "    frequency: frequency of the forecasts (weekly or daily)\n",
    "    paths: paths from the \"Constants\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446a2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(date_min, date_max, frequency, paths = paths):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    dataset = []\n",
    "    while current_date <= end_date:\n",
    "        cdate = current_date.strftime('%Y%m%d')\n",
    "        filename = paths[\"data\"] + cdate[0:4] + \"/\" + cdate[4:6] + \"/\" + \"Dataset_\" + cdate + \".nc\"\n",
    "        if os.path.isfile(filename):\n",
    "            dataset.append(filename)\n",
    "        #\n",
    "        if frequency == \"daily\":\n",
    "            current_date = current_date + datetime.timedelta(days = 1)\n",
    "        elif frequency == \"weekly\":\n",
    "            current_date = current_date + datetime.timedelta(days = 7)\n",
    "    #\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd056f3",
   "metadata": {},
   "source": [
    "extract_variables function\n",
    "\n",
    "    dataset: dataset created using the function \"extract dataset\"\n",
    "    list_variables: list_variables from the \"Constants\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6833180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_variables(dataset, list_variables = list_variables):\n",
    "    nc = netCDF4.Dataset(dataset[0], \"r\")\n",
    "    for var in nc.variables:\n",
    "        if (var in list_variables[\"geolocation\"]) == False:\n",
    "            list_variables[\"data\"].append(var)\n",
    "    #\n",
    "    list_variables[\"data\"].append(\"initial_bias\")\n",
    "    list_variables[\"data\"].append(\"TOPAZ_bias_corrected\")\n",
    "    #\n",
    "    nc.close()\n",
    "    return(list_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169a061",
   "metadata": {},
   "source": [
    "extract_stats function\n",
    "   \n",
    "    dataset: dataset created using the function \"extract dataset\"\n",
    "    variable_name: name of the variable which is going to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224da4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats(dataset, variable_name):\n",
    "    Stats = {}\n",
    "    #\n",
    "    if variable_name == \"LSM\":\n",
    "        nc = netCDF4.Dataset(dataset[0], \"r\")\n",
    "        field_conc = nc.variables[\"LSM\"][:,:]\n",
    "        nc.close()\n",
    "        #\n",
    "        Stats[\"min\"] = np.nanmin(field_conc)\n",
    "        Stats[\"max\"] = np.nanmax(field_conc)\n",
    "        Stats[\"std\"] = np.nanstd(field_conc)\n",
    "        Stats[\"mean\"] = np.nanmean(field_conc)\n",
    "    #\n",
    "    elif (variable_name == \"initial_bias\") or (variable_name == \"TOPAZ_bias_corrected\"):\n",
    "        for i, fi in enumerate(dataset):\n",
    "            nc = netCDF4.Dataset(fi, \"r\")\n",
    "            TOPAZ_SIC = nc.variables[\"TOPAZ_SIC\"][:,:,:]\n",
    "            SICobs_AMSR2 = nc.variables[\"SICobs_AMSR2_SIC\"][:,:]\n",
    "            ini_bias = np.expand_dims(TOPAZ_SIC[0,:,:] - SICobs_AMSR2, axis = 0)\n",
    "            #\n",
    "            if variable_name == \"initial_bias\":\n",
    "                field = np.copy(ini_bias)\n",
    "            elif variable_name == \"TOPAZ_bias_corrected\":\n",
    "                ini_bias_3D = np.repeat(ini_bias, 10, axis = 0)\n",
    "                field = np.expand_dims(TOPAZ_SIC - ini_bias_3D, axis = 0)\n",
    "                field[field < 0] = 0\n",
    "                field[field > 100] = 100\n",
    "            #\n",
    "            if i == 0:\n",
    "                field_conc = np.copy(field)\n",
    "            else:\n",
    "                field_conc = np.concatenate((field_conc, field), axis = 0)\n",
    "            nc.close()\n",
    "        #\n",
    "        Stats[\"min\"] = np.nanmin(field_conc)\n",
    "        Stats[\"max\"] = np.nanmax(field_conc)\n",
    "        Stats[\"std\"] = np.nanstd(field_conc)\n",
    "        Stats[\"mean\"] = np.nanmean(field_conc)\n",
    "    #\n",
    "    elif (\"ECMWF\" in variable_name) or (variable_name == \"TOPAZ_u_cum\") or (variable_name == \"TOPAZ_v_cum\") or (variable_name == \"TOPAZ_temperature_cum\"):\n",
    "        for i, fi in enumerate(dataset):\n",
    "            nc = netCDF4.Dataset(fi, \"r\")\n",
    "            field = np.expand_dims(nc.variables[variable_name][:,:,:], axis = 0)\n",
    "            #\n",
    "            if i == 0:\n",
    "                field_conc = np.copy(field)\n",
    "            else:\n",
    "                field_conc = np.concatenate((field_conc, field), axis = 0)\n",
    "            nc.close()\n",
    "            #\n",
    "        for lt in range(0, len(field[0,:,0,0])):\n",
    "            if lt == 0:\n",
    "                Stats[\"min\"] = np.nanmin(field_conc[:,lt,:,:]) \n",
    "                Stats[\"max\"] = np.nanmax(field_conc[:,lt,:,:])\n",
    "                Stats[\"std\"] = np.nanstd(field_conc[:,lt,:,:])\n",
    "                Stats[\"mean\"] = np.nanmean(field_conc[:,lt,:,:])\n",
    "            else:\n",
    "                Stats[\"min\"] = np.hstack((Stats[\"min\"], np.nanmin(field_conc[:,lt,:,:])))\n",
    "                Stats[\"max\"] = np.hstack((Stats[\"max\"], np.nanmax(field_conc[:,lt,:,:])))\n",
    "                Stats[\"std\"] = np.hstack((Stats[\"std\"], np.nanstd(field_conc[:,lt,:,:])))\n",
    "                Stats[\"mean\"] = np.hstack((Stats[\"mean\"], np.nanmean(field_conc[:,lt,:,:])))\n",
    "    #\n",
    "    else:\n",
    "        for i, fi in enumerate(dataset):\n",
    "            nc = netCDF4.Dataset(fi, \"r\")\n",
    "            if i == 0:\n",
    "                vardim = nc.variables[variable_name].ndim\n",
    "                LSM = nc.variables[\"LSM\"][:,:]\n",
    "                if vardim == 3:\n",
    "                    LSM = np.expand_dims(LSM, axis = 0)\n",
    "                    LSM = np.repeat(LSM, 10, axis = 0)\n",
    "                LSM = np.ndarray.flatten(LSM)\n",
    "            #\n",
    "            if vardim == 2:\n",
    "                field_flat = np.ndarray.flatten(nc.variables[variable_name][:,:])\n",
    "            elif vardim == 3:\n",
    "                field_flat = np.ndarray.flatten(nc.variables[variable_name][:,:,:])\n",
    "            #\n",
    "            field_flat = field_flat[LSM == 1]\n",
    "            #   \n",
    "            if i == 0:\n",
    "                field_conc = np.copy(field_flat)\n",
    "            else:\n",
    "                field_conc = np.hstack((field_conc, field_flat))\n",
    "            #\n",
    "            nc.close()\n",
    "        #\n",
    "        Stats[\"min\"] = np.nanmin(field_conc)\n",
    "        Stats[\"max\"] = np.nanmax(field_conc)\n",
    "        Stats[\"std\"] = np.nanstd(field_conc)\n",
    "        Stats[\"mean\"] = np.nanmean(field_conc)\n",
    "    #\n",
    "    return(Stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533522e7",
   "metadata": {},
   "source": [
    "write_hdf5 function\n",
    "\n",
    "    Stats: output of the function \"extract_stats\"\n",
    "    date_min: date_min from the \"Constants\" section\n",
    "    date_max: date_max from the \"Constants\" section\n",
    "    frequency: frequency from the \"Constants\" section\n",
    "    paths: paths from the \"Constants\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcfc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hdf5(Stats, date_min = date_min, date_max = date_max, frequency = frequency, paths = paths):\n",
    "    filename = paths[\"output\"] + \"Stats_standardization_\" + date_min + \"_\" + date_max + \"_\" + frequency + \".h5\"\n",
    "    hf = h5py.File(filename, 'w')\n",
    "    for var in Stats:\n",
    "        for st in Stats[var]:\n",
    "            output_var = var + \"_\" + st\n",
    "            hf.create_dataset(output_var, data = Stats[var][st])\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b9635",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c03f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset) 408\n",
      "LSM\n",
      "TOPAZ_SIC\n",
      "TOPAZ_hice\n",
      "TOPAZ_u_cum\n",
      "TOPAZ_v_cum\n",
      "ECMWF_T2M_cum\n",
      "ECMWF_wind_x_cum\n",
      "ECMWF_wind_y_cum\n",
      "SICobs_SIC\n",
      "SICobs_trend\n",
      "TARGET_SIC\n",
      "TARGET_SIE_10\n",
      "TARGET_SIE_20\n",
      "TARGET_TOPAZ_error\n",
      "initial_bias\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dataset = extract_dataset(date_min, date_max, frequency = frequency, paths = paths)\n",
    "print(\"len(dataset)\", len(dataset))\n",
    "list_variables =  extract_variables(dataset, list_variables = list_variables)\n",
    "#\n",
    "Stats = {}\n",
    "for var in list_variables[\"data\"]:\n",
    "    print(var)\n",
    "    Stats[var] = extract_stats(dataset = dataset, variable_name = var)\n",
    "#\n",
    "write_hdf5(Stats, date_min = date_min, date_max = date_max, paths = paths)\n",
    "#\n",
    "t1 = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaTF",
   "language": "python",
   "name": "mycondatf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
