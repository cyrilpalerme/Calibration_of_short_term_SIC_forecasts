{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde32bb3-4235-4913-aa1e-ab0d5049bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  []\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m physical_devices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(physical_devices)\n\u001b[0;32m---> 14\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mset_memory_growth(\u001b[43mphysical_devices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     16\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import netCDF4\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975514e-ee1d-433d-a0f2-a68589cdbeb5",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0013a8b-1cc8-437a-bc55-6933f7a33d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"SIC_Attention_Res_UNet\"\n",
    "lead_time = 0\n",
    "#\n",
    "function_path = \"/lustre/storeB/users/cyrilp/COSI/Scripts/Predictor_importances/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Data_generator_Pred_permutation import *\n",
    "function_path = \"/lustre/storeB/users/cyrilp/COSI/Scripts/Models/\" + experiment_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Attention_Res_UNet import *\n",
    "#\n",
    "date_min_test = \"20220101\"\n",
    "date_max_test = \"20221231\"\n",
    "#\n",
    "paths = {}\n",
    "paths[\"SDAP\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/SDAP/OSISAF/Without_coastlines/Gaussian_filter_0km/SDAP_2012_2021/\"\n",
    "paths[\"training\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Land_free_ocean/\"\n",
    "paths[\"standard\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Standardization/\"\n",
    "paths[\"model_weights\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Model_weights/\" + experiment_name + \"/\"\n",
    "paths[\"ice_edge_lengths\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/AMSR2_ice_edge_length/\"\n",
    "paths[\"prediction_scores\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Predictor_importances/\" + experiment_name + \"/lead_time_\" + str(lead_time) + \"_days/scores/\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "file_standardization = paths[\"standard\"] + \"Stats_standardization_20130103_20201231_weekly.h5\"\n",
    "file_model_weights = paths[\"model_weights\"] + \"UNet_leadtime_\" + str(lead_time) + \"_days.h5\"\n",
    "#\n",
    "grid_cell_area = 12500 ** 2  # m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbc863-10ab-420a-9e5f-3fc640cf0081",
   "metadata": {},
   "source": [
    "# Land sea mask\n",
    "    1: Ocean\n",
    "    0: Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6011e0-6249-4840-a3a3-45af2c9be507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_land_sea_mask(paths = paths, start_date = \"20180104\"):\n",
    "    filename = paths[\"training\"] + start_date[0:4] + \"/\" + start_date[4:6] + \"/\" + \"Dataset_\" + start_date + \".nc\"\n",
    "    nc = netCDF4.Dataset(filename, \"r\")\n",
    "    LSM = nc.variables[\"LSM\"][:,:] \n",
    "    nc.close()\n",
    "    return(LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d439d1-4601-43e3-bb7a-66b5767936c8",
   "metadata": {},
   "source": [
    "# U-Net parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69fdef-2b24-40ab-9224-6a123c721c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LSM = load_land_sea_mask()\n",
    "#\n",
    "list_predictors = [\"LSM\", \"TOPAZ_SIC\", \"ECMWF_T2M_cum\", \"ECMWF_wind_x_cum\", \"ECMWF_wind_y_cum\", \"initial_bias\", \"SICobs_AMSR2_SIC\", \"SICobs_AMSR2_trend\"]\n",
    "list_targets = [\"TARGET_AMSR2_SIC\"]\n",
    "#\n",
    "model_params = {\"list_predictors\": list_predictors,\n",
    "                \"list_targets\": list_targets, \n",
    "                \"patch_dim\": (544, 544),\n",
    "                \"batch_size\": 4,\n",
    "                \"n_filters\": [32, 64, 128, 256, 512, 1024],\n",
    "                \"activation\": \"relu\",\n",
    "                \"kernel_initializer\": \"he_normal\",\n",
    "                \"batch_norm\": True,\n",
    "                \"pooling_type\": \"Average\",\n",
    "                \"dropout\": 0,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0479810-2203-4cc1-8135-09aca9e63beb",
   "metadata": {},
   "source": [
    "# make_list_dates function\n",
    "    date_min: earliest date of the period (\"YYYYMMDD\")\n",
    "    date_max: latest date of the period (\"YYYYMMDD\")\n",
    "    frequency: \"daily\" or \"weekly\"\n",
    "    path_data: path where the data are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e46c30-7dbc-4799-b09f-e387f6a8ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max, frequency, path_data):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        filename = path_data + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"Dataset_\" + date_str + \".nc\"\n",
    "        if os.path.isfile(filename):\n",
    "            nc = netCDF4.Dataset(filename, \"r\")\n",
    "            TARGET_AMSR2_SIC = nc.variables[\"TARGET_AMSR2_SIC\"][lead_time,:,:]\n",
    "            nc.close()\n",
    "            if np.sum(np.isnan(TARGET_AMSR2_SIC)) == 0:\n",
    "                list_dates.append(date_str)\n",
    "        #\n",
    "        if frequency == \"daily\":\n",
    "            current_date = current_date + datetime.timedelta(days = 1)\n",
    "        elif frequency == \"weekly\":\n",
    "            current_date = current_date + datetime.timedelta(days = 7)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f7d71-42be-45ed-a766-3d46e8be8fb5",
   "metadata": {},
   "source": [
    "# Standardization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b92c8d-01c2-4af7-bc65-5ee65b85eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standardization_data(file_standardization):\n",
    "    standard = {}\n",
    "    hf = h5py.File(file_standardization, \"r\")\n",
    "    for var in hf:\n",
    "        if \"ECMWF\" in var:\n",
    "            standard[var] = np.array(hf[var])[lead_time]\n",
    "        else:\n",
    "            standard[var] = hf[var][()]\n",
    "    hf.close()\n",
    "    return(standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e8436-ec43-4f0d-af56-1853569bcf33",
   "metadata": {},
   "source": [
    "# Extract evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2a96b-8b3a-4801-a5b9-d12342214721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eval_data(start_date, lead_time):\n",
    "    Eval_data = {}\n",
    "    filename = paths[\"training\"] + start_date[0:4] + \"/\" + start_date[4:6] + \"/\" + \"Dataset_\" + start_date + \".nc\"\n",
    "    nc = netCDF4.Dataset(filename)\n",
    "    #\n",
    "    for var in [\"x\", \"y\", \"lat\", \"lon\", \"TARGET_AMSR2_SIC\", \"TARGET_AMSR2_SIE_10\", \"TARGET_AMSR2_SIE_20\"]:\n",
    "        if nc.variables[var].ndim == 1:\n",
    "            Eval_data[var] = nc.variables[var][:]\n",
    "        elif nc.variables[var].ndim == 2:\n",
    "            Eval_data[var] = nc.variables[var][:,:]\n",
    "        elif nc.variables[var].ndim == 3:\n",
    "            Eval_data[var] = nc.variables[var][lead_time,:,:]\n",
    "    #\n",
    "    Eval_data[\"TARGET_AMSR2_SIE_15\"] = np.zeros(np.shape(Eval_data[\"TARGET_AMSR2_SIC\"]))\n",
    "    Eval_data[\"TARGET_AMSR2_SIE_15\"][Eval_data[\"TARGET_AMSR2_SIC\"] >= 15] = 1\n",
    "    #    \n",
    "    target_date = (datetime.datetime.strptime(start_date, \"%Y%m%d\") + datetime.timedelta(days = lead_time)).strftime(\"%Y%m%d\")\n",
    "    file_ice_edge_lengths = paths[\"ice_edge_lengths\"] + target_date[0:4] + \"/\" + target_date[4:6] + \"/\" + \"Ice_edge_lengths_\" + target_date + \".h5\"\n",
    "    hf = h5py.File(file_ice_edge_lengths, \"r\")\n",
    "    for var in hf:\n",
    "        Eval_data[var] = np.array(hf[var])\n",
    "    hf.close() \n",
    "    nc.close()\n",
    "    #\n",
    "    if int(start_date[0:4]) >= 2022:\n",
    "        previous_day = (datetime.datetime.strptime(start_date, \"%Y%m%d\") - datetime.timedelta(days = 1)).strftime(\"%Y%m%d\")\n",
    "        filename_SDAP = paths[\"SDAP\"] + previous_day[0:4] + \"/\" + previous_day[4:6] + \"/\" + \"SDAP_\" + previous_day + \".nc\"\n",
    "        nc_SDAP = netCDF4.Dataset(filename_SDAP, \"r\")\n",
    "        Eval_data[\"SDAP10\"] = nc_SDAP.variables[\"SDAP10\"][lead_time + 1,:,:]\n",
    "        Eval_data[\"SDAP15\"] = nc_SDAP.variables[\"SDAP15\"][lead_time + 1,:,:]\n",
    "        Eval_data[\"SDAP20\"] = nc_SDAP.variables[\"SDAP20\"][lead_time + 1,:,:]\n",
    "        nc_SDAP.close()\n",
    "    #\n",
    "    return(Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8eb8b-4e7a-4c6a-a495-f1afa2a1a724",
   "metadata": {},
   "source": [
    "# Functions for calculating SIC from predictions, and for making binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80371f72-5c8a-4e43-a2e0-8e524572b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIC_from_normalized_SIC(variable_name, field, standard):\n",
    "    Predicted_SIC = field * (standard[variable_name + \"_max\"] - standard[variable_name + \"_min\"]) + standard[variable_name + \"_min\"]\n",
    "    Predicted_SIC[Predicted_SIC > 100] = 100\n",
    "    Predicted_SIC[Predicted_SIC < 0] = 0\n",
    "    return(Predicted_SIC)\n",
    "#\n",
    "def SIC_from_normalized_model_error(variable_name, field, standard, si_model_SIC):\n",
    "    model_error = field * (standard[variable_name + \"_max\"] - standard[variable_name + \"_min\"]) + standard[variable_name + \"_min\"]\n",
    "    Predicted_SIC = si_model_SIC - model_error\n",
    "    Predicted_SIC[Predicted_SIC > 100] = 100\n",
    "    Predicted_SIC[Predicted_SIC < 0] = 0\n",
    "    return(Predicted_SIC)\n",
    "#\n",
    "def binary_classification(field, threshold):\n",
    "    output = np.zeros(np.shape(field))\n",
    "    output[field > threshold] = 1\n",
    "    return(output)\n",
    "#\n",
    "def RMSE(SIC_forecasts, SIC_observations, LSM):\n",
    "    SIC_forecasts = np.ndarray.flatten(SIC_forecasts[LSM == 1])\n",
    "    SIC_observations = np.ndarray.flatten(SIC_observations[LSM == 1])\n",
    "    MSE = np.sum((SIC_forecasts - SIC_observations) ** 2) / len(SIC_observations)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68f3cd-4d10-4ee5-966c-39462c4066b6",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f0138-f854-47e0-b2b4-e88d4d22ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ice_edge_lengths(file_ice_edge_lengths):\n",
    "    Dataset = {}\n",
    "    hf = h5py.File(file_ice_edge_lengths, \"r\")\n",
    "    for var in hf:\n",
    "        Dataset[var] = np.array(hf[var])\n",
    "    hf.close()\n",
    "    return(Dataset)\n",
    "#\n",
    "def IIEE(SIE_obs, SIE_forecast, grid_cell_area):\n",
    "    Flag_SIE = np.full(np.shape(SIE_obs), np.nan)\n",
    "    Flag_SIE[SIE_forecast == SIE_obs] = 0\n",
    "    Flag_SIE[SIE_forecast < SIE_obs] = -1\n",
    "    Flag_SIE[SIE_forecast > SIE_obs] = 1\n",
    "    Underestimation = np.sum(Flag_SIE == -1) * grid_cell_area\n",
    "    Overestimation = np.sum(Flag_SIE == 1) * grid_cell_area\n",
    "    IIEE_metric = Underestimation + Overestimation\n",
    "    return(IIEE_metric, Underestimation, Overestimation)\n",
    "#\n",
    "def SPS(SIP_obs, SIP_forecast, grid_cell_area):\n",
    "    SPS_metric = np.nansum(grid_cell_area * (SIP_forecast - SIP_obs)**2)\n",
    "    return(SPS_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89e329-e55a-4962-8c4a-5540128a1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification_scores(Pred_data, Eval_data, start_date, LSM, grid_cell_area = grid_cell_area):\n",
    "    day_of_year = int(datetime.datetime.strptime(start_date, \"%Y%m%d\").strftime('%j'))\n",
    "    #\n",
    "    ML = {}\n",
    "    ML[\"SIE_10\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 10)\n",
    "    ML[\"SIE_15\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 15)\n",
    "    ML[\"SIE_20\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 20)\n",
    "    #\n",
    "    Metrics = {}\n",
    "    Metrics[\"start_date\"] = start_date\n",
    "    Metrics[\"Ice_edge_length_SIC10\"] = Eval_data[\"Ice_edge_lengths_SIC10\"]\n",
    "    Metrics[\"Ice_edge_length_SIC15\"] = Eval_data[\"Ice_edge_lengths_SIC15\"]\n",
    "    Metrics[\"Ice_edge_length_SIC20\"] = Eval_data[\"Ice_edge_lengths_SIC20\"]\n",
    "    #\n",
    "    Metrics[\"RMSE_ML\"] = RMSE(Pred_data[\"Predicted_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"IIEElength_10_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], ML[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]\n",
    "    Metrics[\"IIEElength_15_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], ML[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]\n",
    "    Metrics[\"IIEElength_20_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], ML[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    #\n",
    "    for var in Metrics:\n",
    "        if var != \"start_date\":\n",
    "            if \"RMSE\" in var:\n",
    "                Metrics[var] = np.round(Metrics[var], 3)\n",
    "            else:\n",
    "                Metrics[var] = np.round(Metrics[var])\n",
    "    #\n",
    "    return(Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e2c35-09cc-47d0-bd14-c9783a336aff",
   "metadata": {},
   "source": [
    "# Write_scores function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b04861-2489-425e-abf0-96420f0879b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(Metrics, permuted_predictor, paths = paths):\n",
    "    header = \"\"\n",
    "    scores = \"\"\n",
    "    for var in Metrics:\n",
    "        header = header + \"\\t\" + var   \n",
    "        scores = scores + \"\\t\" + str(Metrics[var]) \n",
    "    #\n",
    "    output_file = paths[\"prediction_scores\"] + \"Scores_\" + date_min_test + \"_\" + date_max_test + \"_permuted_predictor_\" + permuted_predictor + \".txt\"\n",
    "    if start_date == date_min_test:\n",
    "        if os.path.isfile(output_file) == True:\n",
    "            os.system(\"rm \" + output_file)\n",
    "    #\n",
    "    if os.path.isfile(output_file) == False:\n",
    "        output = open(output_file, 'a')\n",
    "        output.write(header + \"\\n\")\n",
    "        output.close()\n",
    "    #\n",
    "    output = open(output_file, 'a')\n",
    "    output.write(scores + \"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795d49d-3aaf-4d23-b8ff-2822be93ae64",
   "metadata": {},
   "source": [
    "# Make predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e985e-3dbb-45eb-a585-e24702b62531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(start_date, list_dates_test, model, standard, LSM, permuted_predictor):\n",
    "    Eval_data = extract_eval_data(start_date, lead_time)\n",
    "    #\n",
    "    permuted_date = list_dates_test[np.random.randint(low = 0, high = len(list_dates_test))]\n",
    "    if permuted_date == start_date:\n",
    "        while permuted_date == start_date:\n",
    "            permuted_date = list_dates_test[np.random.randint(low = 0, high = len(list_dates_test))]\n",
    "    #\n",
    "    params_test = {\"list_predictors\": model_params[\"list_predictors\"],\n",
    "                    \"list_labels\": model_params[\"list_targets\"],\n",
    "                    \"list_dates\": [start_date],\n",
    "                    \"lead_time\": lead_time,\n",
    "                    \"standard\": standard,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"path_data\": paths[\"training\"],\n",
    "                    \"dim\": model_params[\"patch_dim\"],\n",
    "                    \"shuffle\": False,\n",
    "                    \"permuted_predictor\": permuted_predictor,\n",
    "                    \"permuted_date\": permuted_date,\n",
    "                    }\n",
    "    #\n",
    "    test_generator = Data_generator_Pred_permutation(**params_test)\n",
    "    predictions = np.squeeze(model.predict(test_generator))\n",
    "    if list_targets[0] == \"TARGET_AMSR2_TOPAZ_error\":\n",
    "        predictions = SIC_from_normalized_model_error(\"TARGET_AMSR2_TOPAZ_error\", predictions, standard, Eval_data[\"TOPAZ_SIC\"])\n",
    "    elif list_targets[0] == \"TARGET_AMSR2_SIC\":\n",
    "        predictions = SIC_from_normalized_SIC(\"TARGET_AMSR2_SIC\", predictions, standard)\n",
    "    predictions[:,:][LSM == 0] = 0\n",
    "    # \n",
    "    Pred_data = {}\n",
    "    Pred_data[\"Predicted_SIC\"] = np.copy(predictions[:,:])\n",
    "    #\n",
    "    return(Pred_data, Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf9008-814a-4642-ba32-647b281af18c",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298d094-7ba8-47ef-b03b-0f8ab1ffdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = load_standardization_data(file_standardization)\n",
    "list_dates_test = make_list_dates(date_min_test, date_max_test, frequency = \"daily\", path_data = paths[\"training\"])\n",
    "#\n",
    "unet_model = Att_Res_UNet(**model_params).make_unet_model()\n",
    "unet_model.load_weights(file_model_weights)\n",
    "#print(unet_model.summary())\n",
    "#\n",
    "Scores = {}\n",
    "list_predictors_extended = list_predictors + [\"wind\"]\n",
    "for pred in list_predictors_extended:\n",
    "    print(pred)\n",
    "    for sd, start_date in enumerate(list_dates_test):\n",
    "        try:\n",
    "            Pred_data, Eval_data = make_predictions(start_date = start_date, list_dates_test = list_dates_test, model = unet_model, standard = standard, LSM = LSM, permuted_predictor = pred)\n",
    "            Metrics = verification_scores(Pred_data, Eval_data, start_date, LSM, grid_cell_area = grid_cell_area)\n",
    "            save_scores(Metrics, permuted_predictor = pred, paths = paths)\n",
    "        except:\n",
    "            pass\n",
    "#\n",
    "t1 = time.time()\n",
    "dt = t1 - t0\n",
    "#\n",
    "print(\"Predictions made ! Time: \", dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaTF",
   "language": "python",
   "name": "mycondatf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
