{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e187db36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmixed_precision\u001b[38;5;241m.\u001b[39mset_global_policy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_float16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import netCDF4\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37f274",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"SIC_Res_UNet\"\n",
    "lead_time = 0\n",
    "#\n",
    "function_path = \"/lustre/storeB/users/cyrilp/COSI/Scripts/Models/\" + experiment_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Data_generator_UNet import *\n",
    "from ResUNet import *\n",
    "#\n",
    "date_min_test = \"20220101\"\n",
    "date_max_test = \"20221231\"\n",
    "#\n",
    "paths = {}\n",
    "paths[\"SDAP\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/SDAP/OSISAF/Without_coastlines/Gaussian_filter_0km/SDAP_2012_2021/\"\n",
    "paths[\"anomaly_persistence\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Anomaly_persistence_forecasts/AMSR2/\"\n",
    "paths[\"training\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Land_free_ocean/\"\n",
    "paths[\"standard\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Standardization/\"\n",
    "paths[\"model_weights\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Model_weights/\" + experiment_name + \"/\"\n",
    "paths[\"ice_edge_lengths\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/AMSR2_ice_edge_length/\"\n",
    "paths[\"predictions_netCDF\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Predictions/\" + experiment_name + \"/lead_time_\" + str(lead_time) + \"_days/netCDF/\"\n",
    "paths[\"prediction_scores\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Predictions/\" + experiment_name + \"/lead_time_\" + str(lead_time) + \"_days/scores/\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "file_standardization = paths[\"standard\"] + \"Stats_standardization_20130103_20201231_weekly.h5\"\n",
    "file_model_weights = paths[\"model_weights\"] + \"UNet_leadtime_\" + str(lead_time) + \"_days.h5\"\n",
    "#\n",
    "grid_cell_area = 12500 ** 2  # m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3d337",
   "metadata": {},
   "source": [
    "# U-Net parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predictors = [\"LSM\", \"TOPAZ_SIC\", \"ECMWF_T2M_cum\", \"ECMWF_wind_x_cum\", \"ECMWF_wind_y_cum\", \"initial_bias\", \"SICobs_AMSR2_SIC\", \"SICobs_AMSR2_trend\"]\n",
    "list_targets = [\"TARGET_AMSR2_SIC\"]\n",
    "#\n",
    "model_params = {\"list_predictors\": list_predictors,\n",
    "                \"list_targets\": list_targets, \n",
    "                \"patch_dim\": (544, 544),\n",
    "                \"batch_size\": 4,\n",
    "                \"n_filters\": [32, 64, 128, 256, 512, 1024],\n",
    "                \"activation\": \"relu\",\n",
    "                \"kernel_initializer\": \"he_normal\",\n",
    "                \"batch_norm\": True,\n",
    "                \"pooling_type\": \"Average\",\n",
    "                \"dropout\": 0,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3a427",
   "metadata": {},
   "source": [
    "# Load Land-Sea mask\n",
    "\n",
    "    1: Ocean\n",
    "    0: Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_land_sea_mask(paths = paths, start_date = \"20180104\"):\n",
    "    filename = paths[\"training\"] + start_date[0:4] + \"/\" + start_date[4:6] + \"/\" + \"Dataset_\" + start_date + \".nc\"\n",
    "    nc = netCDF4.Dataset(filename, \"r\")\n",
    "    LSM = nc.variables[\"LSM\"][:,:] \n",
    "    nc.close()\n",
    "    return(LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93260b",
   "metadata": {},
   "source": [
    "# make_list_dates function\n",
    "\n",
    "    date_min: earliest date of the period (\"YYYYMMDD\")\n",
    "    date_max: latest date of the period (\"YYYYMMDD\")\n",
    "    frequency: \"daily\" or \"weekly\"\n",
    "    path_data: path where the data are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max, frequency, path_data, lead_time = lead_time):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        filename = path_data + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"Dataset_\" + date_str + \".nc\"\n",
    "        if os.path.isfile(filename):\n",
    "            nc = netCDF4.Dataset(filename, \"r\")\n",
    "            TARGET_AMSR2_SIC = nc.variables[\"TARGET_AMSR2_SIC\"][lead_time,:,:]\n",
    "            nc.close()\n",
    "            if np.sum(np.isnan(TARGET_AMSR2_SIC)) == 0:\n",
    "                list_dates.append(date_str)\n",
    "        #\n",
    "        if frequency == \"daily\":\n",
    "            current_date = current_date + datetime.timedelta(days = 1)\n",
    "        elif frequency == \"weekly\":\n",
    "            current_date = current_date + datetime.timedelta(days = 7)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73caeb",
   "metadata": {},
   "source": [
    "# Standardization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standardization_data(file_standardization):\n",
    "    standard = {}\n",
    "    hf = h5py.File(file_standardization, \"r\")\n",
    "    for var in hf:\n",
    "        if \"ECMWF\" in var:\n",
    "            standard[var] = np.array(hf[var])[lead_time]\n",
    "        else:\n",
    "            standard[var] = hf[var][()]\n",
    "    hf.close()\n",
    "    return(standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ecd8c",
   "metadata": {},
   "source": [
    "# Extract evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63dc3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eval_data(start_date, lead_time):\n",
    "    previous_day = (datetime.datetime.strptime(start_date, \"%Y%m%d\") - datetime.timedelta(days = 1)).strftime(\"%Y%m%d\")\n",
    "    #\n",
    "    Eval_data = {}\n",
    "    filename_training = paths[\"training\"] + start_date[0:4] + \"/\" + start_date[4:6] + \"/\" + \"Dataset_\" + start_date + \".nc\"\n",
    "    nc_training = netCDF4.Dataset(filename_training, \"r\")\n",
    "    #\n",
    "    for var in [\"x\", \"y\", \"lat\", \"lon\", \"TOPAZ_SIC\", \"TARGET_AMSR2_SIC\", \"TARGET_AMSR2_SIE_10\", \"TARGET_AMSR2_SIE_20\", \"SICobs_AMSR2_SIC\", \"TOPAZ_bias_corrected\"]:\n",
    "        if var == \"TOPAZ_bias_corrected\":\n",
    "            ini_bias = nc_training.variables[\"TOPAZ_SIC\"][0,:,:] - nc_training.variables[\"SICobs_AMSR2_SIC\"][:,:]\n",
    "            T4_SIC = nc_training.variables[\"TOPAZ_SIC\"][lead_time,:,:]\n",
    "            T4_SIC_bias_corrected = T4_SIC - ini_bias\n",
    "            T4_SIC_bias_corrected[T4_SIC_bias_corrected > 100] = 100 \n",
    "            T4_SIC_bias_corrected[T4_SIC_bias_corrected < 0] = 0\n",
    "            Eval_data[var] = np.copy(T4_SIC_bias_corrected)\n",
    "        else:\n",
    "            if nc_training.variables[var].ndim == 1:\n",
    "                Eval_data[var] = nc_training.variables[var][:]\n",
    "            elif nc_training.variables[var].ndim == 2:\n",
    "                Eval_data[var] = nc_training.variables[var][:,:]\n",
    "            elif nc_training.variables[var].ndim == 3:\n",
    "                Eval_data[var] = nc_training.variables[var][lead_time,:,:]\n",
    "    nc_training.close()\n",
    "    Eval_data[\"TARGET_AMSR2_SIE_15\"] = np.zeros(np.shape(Eval_data[\"TARGET_AMSR2_SIC\"]))\n",
    "    Eval_data[\"TARGET_AMSR2_SIE_15\"][Eval_data[\"TARGET_AMSR2_SIC\"] >= 15] = 1\n",
    "    #\n",
    "    filename_anomaly_persistence = paths[\"anomaly_persistence\"] + previous_day[0:4] + \"/\" + previous_day[4:6] + \"/\" + \"SIC_\" + previous_day + \".nc\"\n",
    "    nc_anomaly_persistence = netCDF4.Dataset(filename_anomaly_persistence, \"r\")\n",
    "    Eval_data[\"Anomaly_persistence_SIC\"] = nc_anomaly_persistence.variables[\"SIC\"][lead_time + 1,:,:]\n",
    "    nc_anomaly_persistence.close()\n",
    "    #\n",
    "    target_date = (datetime.datetime.strptime(start_date, \"%Y%m%d\") + datetime.timedelta(days = lead_time)).strftime(\"%Y%m%d\")\n",
    "    file_ice_edge_lengths = paths[\"ice_edge_lengths\"] + target_date[0:4] + \"/\" + target_date[4:6] + \"/\" + \"Ice_edge_lengths_\" + target_date + \".h5\"\n",
    "    hf = h5py.File(file_ice_edge_lengths, \"r\")\n",
    "    for var in hf:\n",
    "        Eval_data[var] = np.array(hf[var])\n",
    "    hf.close()    \n",
    "    #\n",
    "    if int(start_date[0:4]) >= 2022:\n",
    "        filename_SDAP = paths[\"SDAP\"] + previous_day[0:4] + \"/\" + previous_day[4:6] + \"/\" + \"SDAP_\" + previous_day + \".nc\"\n",
    "        nc_SDAP = netCDF4.Dataset(filename_SDAP, \"r\")\n",
    "        Eval_data[\"SDAP10\"] = nc_SDAP.variables[\"SDAP10\"][lead_time + 1,:,:]\n",
    "        Eval_data[\"SDAP15\"] = nc_SDAP.variables[\"SDAP15\"][lead_time + 1,:,:]\n",
    "        Eval_data[\"SDAP20\"] = nc_SDAP.variables[\"SDAP20\"][lead_time + 1,:,:]\n",
    "        nc_SDAP.close()\n",
    "    #\n",
    "    return(Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0074e3",
   "metadata": {},
   "source": [
    "# Save predictions in netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_in_netCDF(start_date, Pred_data, Eval_data, paths = paths):\n",
    "    file_output = paths[\"predictions_netCDF\"] + \"Predictions_\" + start_date + \".nc\"\n",
    "    output_netcdf = netCDF4.Dataset(file_output, 'w', format = 'NETCDF4')\n",
    "    Outputs = vars()\n",
    "    #\n",
    "    dimensions = [\"x\", \"y\"]\n",
    "    for di in dimensions:\n",
    "        Outputs[di] = output_netcdf.createDimension(di, len(Eval_data[di]))\n",
    "    #\n",
    "    dim_variables = dimensions + [\"lat\", \"lon\"]\n",
    "    for dv in dim_variables:\n",
    "        if Eval_data[dv].ndim == 1:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (dv))\n",
    "            Outputs[dv][:] = Eval_data[dv]   \n",
    "            if dv == \"x\" or dv == \"y\":\n",
    "                Outputs[dv].standard_name = \"projection_\" + dv + \"_coordinate\"\n",
    "                Outputs[dv].units = \"m\"\n",
    "        elif Eval_data[dv].ndim == 2:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (\"y\", \"x\"))\n",
    "            Outputs[dv][:,:] = Eval_data[dv]\n",
    "            if dv == \"lat\":\n",
    "                Outputs[dv].standard_name = \"latitude\"\n",
    "            elif dv == \"lon\":\n",
    "                Outputs[dv].standard_name = \"longitude\"\n",
    "            Outputs[dv].units = \"degrees\"\n",
    "    #\n",
    "    for var in Eval_data:\n",
    "        if \"Ice_edge_lengths_SIC\" in var:\n",
    "            pass\n",
    "        else:\n",
    "            if (var in dim_variables) == False:\n",
    "                Outputs[var] = output_netcdf.createVariable(var, \"d\", (\"y\", \"x\"))\n",
    "                Outputs[var][:,:] = Eval_data[var]\n",
    "                if \"SIC\" in var:\n",
    "                    Outputs[var].standard_name = \"sea ice concentration\"\n",
    "                    Outputs[var].units = \"%\"\n",
    "                elif \"SIE\" in var:\n",
    "                    Outputs[var].standard_name = \"sea ice extent\"\n",
    "                    Outputs[var].units = \"1 if sea ice concentration higher than \" + var[-2:len(var)] + \" %, 0 otherwise\"\n",
    "                elif \"TOPAZ_bias_corrected\" in var:\n",
    "                    Outputs[var].standard_name = \"TOPAZ bias corrected forecasts\"\n",
    "                    Outputs[var].units = \"%\"\n",
    "                elif \"SDAP\" in var:\n",
    "                    Outputs[var].standard_name = \"Spatial damped anomaly persistence forecasts (SDAP) for sea-ice concentration higher than \" + var[4:6] + \" %\"\n",
    "                    Outputs[var].units = \"Fraction\"\n",
    "    #\n",
    "    for var in Pred_data:\n",
    "        Outputs[var] = output_netcdf.createVariable(var, \"d\", (\"y\", \"x\"))\n",
    "        Outputs[var][:,:] = Pred_data[var]\n",
    "        if \"TOPAZ_error\" in var:\n",
    "            Outputs[var].standard_name = \"Predicted SIC from model error\"\n",
    "            Outputs[var].units = \"%\"\n",
    "        elif \"SIC\" in var:\n",
    "            Outputs[var].standard_name = var\n",
    "            Outputs[var].units = \"%\" \n",
    "        elif \"SIE\" in var:\n",
    "            Outputs[var].standard_name = \"Predicted sea ice extent (SIC threshold \" + var[-2:len(var)] + \" %)\"\n",
    "            Outputs[var].units = \"1: ice, 0: ice-free\"   \n",
    "    #\n",
    "    output_netcdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c554d",
   "metadata": {},
   "source": [
    "# Functions for calculating SIC from predictions, and for making binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIC_from_normalized_SIC(variable_name, field, standard):\n",
    "    Predicted_SIC = field * (standard[variable_name + \"_max\"] - standard[variable_name + \"_min\"]) + standard[variable_name + \"_min\"]\n",
    "    Predicted_SIC[Predicted_SIC > 100] = 100\n",
    "    Predicted_SIC[Predicted_SIC < 0] = 0\n",
    "    return(Predicted_SIC)\n",
    "#\n",
    "def SIC_from_normalized_model_error(variable_name, field, standard, si_model_SIC):\n",
    "    model_error = field * (standard[variable_name + \"_max\"] - standard[variable_name + \"_min\"]) + standard[variable_name + \"_min\"]\n",
    "    print(\"model_error\", np.min(model_error), np.max(model_error), np.mean(model_error), np.median(model_error))\n",
    "    Predicted_SIC = si_model_SIC - model_error\n",
    "    Predicted_SIC[Predicted_SIC > 100] = 100\n",
    "    Predicted_SIC[Predicted_SIC < 0] = 0\n",
    "    return(Predicted_SIC)\n",
    "#\n",
    "def binary_classification(field, threshold):\n",
    "    output = np.zeros(np.shape(field))\n",
    "    output[field > threshold] = 1\n",
    "    return(output)\n",
    "#\n",
    "def RMSE(SIC_forecasts, SIC_observations, LSM):\n",
    "    SIC_forecasts = np.ndarray.flatten(SIC_forecasts[LSM == 1])\n",
    "    SIC_observations = np.ndarray.flatten(SIC_observations[LSM == 1])\n",
    "    MSE = np.sum((SIC_forecasts - SIC_observations) ** 2) / len(SIC_observations)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7338b9",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ice_edge_lengths(file_ice_edge_lengths):\n",
    "    Dataset = {}\n",
    "    hf = h5py.File(file_ice_edge_lengths, \"r\")\n",
    "    for var in hf:\n",
    "        Dataset[var] = np.array(hf[var])\n",
    "    hf.close()\n",
    "    return(Dataset)\n",
    "#\n",
    "def IIEE(SIE_obs, SIE_forecast, grid_cell_area):\n",
    "    Flag_SIE = np.full(np.shape(SIE_obs), np.nan)\n",
    "    Flag_SIE[SIE_forecast == SIE_obs] = 0\n",
    "    Flag_SIE[SIE_forecast < SIE_obs] = -1\n",
    "    Flag_SIE[SIE_forecast > SIE_obs] = 1\n",
    "    Underestimation = np.sum(Flag_SIE == -1) * grid_cell_area\n",
    "    Overestimation = np.sum(Flag_SIE == 1) * grid_cell_area\n",
    "    IIEE_metric = Underestimation + Overestimation\n",
    "    return(IIEE_metric, Underestimation, Overestimation)\n",
    "#\n",
    "def SPS(SIP_obs, SIP_forecast, grid_cell_area):\n",
    "    SPS_metric = np.nansum(grid_cell_area * (SIP_forecast - SIP_obs)**2)\n",
    "    return(SPS_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a80758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification_scores(Pred_data, Eval_data, start_date, LSM, grid_cell_area = grid_cell_area):\n",
    "    day_of_year = int(datetime.datetime.strptime(start_date, \"%Y%m%d\").strftime('%j'))\n",
    "    #\n",
    "    ML = {}\n",
    "    ML[\"SIE_10\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 10)\n",
    "    ML[\"SIE_15\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 15)\n",
    "    ML[\"SIE_20\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 20)\n",
    "    #\n",
    "    Persistence = {}\n",
    "    Persistence[\"SIE_10\"] = binary_classification(Eval_data[\"SICobs_AMSR2_SIC\"], 10)\n",
    "    Persistence[\"SIE_15\"] = binary_classification(Eval_data[\"SICobs_AMSR2_SIC\"], 15)\n",
    "    Persistence[\"SIE_20\"] = binary_classification(Eval_data[\"SICobs_AMSR2_SIC\"], 20)\n",
    "    #\n",
    "    TOPAZ = {}\n",
    "    TOPAZ[\"SIE_10\"] = binary_classification(Eval_data[\"TOPAZ_SIC\"], 10)\n",
    "    TOPAZ[\"SIE_15\"] = binary_classification(Eval_data[\"TOPAZ_SIC\"], 15)\n",
    "    TOPAZ[\"SIE_20\"] = binary_classification(Eval_data[\"TOPAZ_SIC\"], 20)\n",
    "    #\n",
    "    TOPAZ_bias_corrected = {}\n",
    "    TOPAZ_bias_corrected[\"SIE_10\"] = binary_classification(Eval_data[\"TOPAZ_bias_corrected\"], 10)\n",
    "    TOPAZ_bias_corrected[\"SIE_15\"] = binary_classification(Eval_data[\"TOPAZ_bias_corrected\"], 15)\n",
    "    TOPAZ_bias_corrected[\"SIE_20\"] = binary_classification(Eval_data[\"TOPAZ_bias_corrected\"], 20)\n",
    "    #\n",
    "    Anomaly_persistence = {}\n",
    "    Anomaly_persistence[\"SIE_10\"] = binary_classification(Eval_data[\"Anomaly_persistence_SIC\"], 10)\n",
    "    Anomaly_persistence[\"SIE_15\"] = binary_classification(Eval_data[\"Anomaly_persistence_SIC\"], 15)\n",
    "    Anomaly_persistence[\"SIE_20\"] = binary_classification(Eval_data[\"Anomaly_persistence_SIC\"], 20)\n",
    "    #\n",
    "    if int(start_date[0:4]) >= 2022:\n",
    "        SDAP = {}\n",
    "        SDAP[\"SIE_10\"] = binary_classification(Eval_data[\"SDAP10\"], 0.5)\n",
    "        SDAP[\"SIE_15\"] = binary_classification(Eval_data[\"SDAP15\"], 0.5)\n",
    "        SDAP[\"SIE_20\"] = binary_classification(Eval_data[\"SDAP20\"], 0.5)\n",
    "    #\n",
    "    Metrics = {}\n",
    "    Metrics[\"start_date\"] = start_date\n",
    "    Metrics[\"Ice_edge_length_SIC10\"] = Eval_data[\"Ice_edge_lengths_SIC10\"]\n",
    "    Metrics[\"Ice_edge_length_SIC15\"] = Eval_data[\"Ice_edge_lengths_SIC15\"]\n",
    "    Metrics[\"Ice_edge_length_SIC20\"] = Eval_data[\"Ice_edge_lengths_SIC20\"]\n",
    "    #\n",
    "    Metrics[\"RMSE_ML\"] = RMSE(Pred_data[\"Predicted_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"RMSE_TOPAZ\"] = RMSE(Eval_data[\"TOPAZ_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"RMSE_TOPAZ_bias_corrected\"] = RMSE(Eval_data[\"TOPAZ_bias_corrected\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"RMSE_Persistence\"] = RMSE(Eval_data[\"SICobs_AMSR2_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"RMSE_Anomaly_persistence\"] =  RMSE(Eval_data[\"Anomaly_persistence_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    #\n",
    "    Metrics[\"IIEElength_10_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], ML[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]\n",
    "    Metrics[\"IIEElength_10_TOPAZ\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], TOPAZ[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]\n",
    "    Metrics[\"IIEElength_10_TOPAZ_bias_corrected\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], TOPAZ_bias_corrected[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]\n",
    "    Metrics[\"IIEElength_10_Persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], Persistence[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]  \n",
    "    Metrics[\"IIEElength_10_Anomaly_persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], Anomaly_persistence[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]\n",
    "    if int(start_date[0:4]) >= 2022:\n",
    "        Metrics[\"IIEElength_10_SDAP\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], SDAP[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"] \n",
    "    #\n",
    "    Metrics[\"IIEElength_15_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], ML[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]\n",
    "    Metrics[\"IIEElength_15_TOPAZ\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], TOPAZ[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]\n",
    "    Metrics[\"IIEElength_15_TOPAZ_bias_corrected\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], TOPAZ_bias_corrected[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]\n",
    "    Metrics[\"IIEElength_15_Persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], Persistence[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]  \n",
    "    Metrics[\"IIEElength_15_Anomaly_persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], Anomaly_persistence[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]\n",
    "    if int(start_date[0:4]) >= 2022:\n",
    "        Metrics[\"IIEElength_15_SDAP\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_15\"], SDAP[\"SIE_15\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC15\"]   \n",
    "    #\n",
    "    Metrics[\"IIEElength_20_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], ML[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    Metrics[\"IIEElength_20_TOPAZ\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], TOPAZ[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    Metrics[\"IIEElength_20_TOPAZ_bias_corrected\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], TOPAZ_bias_corrected[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    Metrics[\"IIEElength_20_Persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], Persistence[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    Metrics[\"IIEElength_20_Anomaly_persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], Anomaly_persistence[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    if int(start_date[0:4]) >= 2022:\n",
    "        Metrics[\"IIEElength_20_SDAP\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_20\"], SDAP[\"SIE_20\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC20\"]\n",
    "    #\n",
    "    for var in Metrics:\n",
    "        if var != \"start_date\":\n",
    "            if \"RMSE\" in var:\n",
    "                Metrics[var] = np.round(Metrics[var], 3)\n",
    "            else:\n",
    "                Metrics[var] = np.round(Metrics[var])\n",
    "    #\n",
    "    return(Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0b8dd",
   "metadata": {},
   "source": [
    "# Write_scores function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(Metrics, paths = paths):\n",
    "    header = \"\"\n",
    "    scores = \"\"\n",
    "    for var in Metrics:\n",
    "        header = header + \"\\t\" + var   \n",
    "        scores = scores + \"\\t\" + str(Metrics[var]) \n",
    "    #\n",
    "    output_file = paths[\"prediction_scores\"] + \"Scores_\" + date_min_test + \"_\" + date_max_test + \".txt\"\n",
    "    if start_date == date_min_test:\n",
    "        if os.path.isfile(output_file) == True:\n",
    "            os.system(\"rm \" + output_file)\n",
    "    #\n",
    "    if os.path.isfile(output_file) == False:\n",
    "        output = open(output_file, 'a')\n",
    "        output.write(header + \"\\n\")\n",
    "        output.close()\n",
    "    #\n",
    "    output = open(output_file, 'a')\n",
    "    output.write(scores + \"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c35d7",
   "metadata": {},
   "source": [
    "# Make predictions functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(start_date, model, standard, LSM):\n",
    "    Eval_data = extract_eval_data(start_date, lead_time)\n",
    "    #\n",
    "    params_test = {\"list_predictors\": model_params[\"list_predictors\"],\n",
    "                    \"list_labels\": model_params[\"list_targets\"],\n",
    "                    \"list_dates\": [start_date],\n",
    "                    \"lead_time\": lead_time,\n",
    "                    \"standard\": standard,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"path_data\": paths[\"training\"],\n",
    "                    \"dim\": model_params[\"patch_dim\"],\n",
    "                    \"shuffle\": False,\n",
    "                    }\n",
    "    #\n",
    "    test_generator = Data_generator(**params_test)\n",
    "    predictions = np.squeeze(model.predict(test_generator))\n",
    "    predictions = SIC_from_normalized_SIC(\"TARGET_AMSR2_SIC\", predictions, standard)\n",
    "    predictions[:,:][LSM == 0] = 0\n",
    "    # \n",
    "    Pred_data = {}\n",
    "    Pred_data[\"Predicted_SIC\"] = np.copy(predictions[:,:])\n",
    "    #\n",
    "    return(Pred_data, Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f854b",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = load_land_sea_mask()\n",
    "standard = load_standardization_data(file_standardization)\n",
    "list_dates_test = make_list_dates(date_min_test, date_max_test, frequency = \"daily\", path_data = paths[\"training\"])\n",
    "#\n",
    "unet_model = Res_UNet(**model_params).make_unet_model()\n",
    "unet_model.load_weights(file_model_weights)\n",
    "#print(unet_model.summary())\n",
    "#\n",
    "Scores = {}\n",
    "for sd, start_date in enumerate(list_dates_test):\n",
    "    print(\"forecast start_date\", start_date)\n",
    "    try:\n",
    "        Pred_data, Eval_data = make_predictions(start_date = start_date, model = unet_model, standard = standard, LSM = LSM)\n",
    "        #save_predictions_in_netCDF(start_date, Pred_data, Eval_data)\n",
    "        Metrics = verification_scores(Pred_data, Eval_data, start_date, LSM, grid_cell_area = grid_cell_area)\n",
    "        save_scores(Metrics, paths = paths)\n",
    "    except:\n",
    "        pass\n",
    "#\n",
    "t1 = time.time()\n",
    "dt = t1 - t0\n",
    "#\n",
    "print(\"Predictions made ! Time: \", dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
