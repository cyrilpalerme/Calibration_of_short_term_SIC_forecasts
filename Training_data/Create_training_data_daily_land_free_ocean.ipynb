{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "41950ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import scipy\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import datetime \n",
    "import sklearn.linear_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f1380",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "be7d5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGE_TASK_ID = 1\n",
    "#\n",
    "date_min = \"20210618\"\n",
    "date_max = \"20221231\"\n",
    "#\n",
    "SIC_trend_period = 5\n",
    "lead_time_max = 10\n",
    "#\n",
    "paths = {}\n",
    "paths[\"output\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/Training/Land_free_ocean/\"\n",
    "paths[\"LSM\"] = \"/lustre/storeB/project/copernicus/svalnav/Data/TOPAZ4/\"\n",
    "paths[\"TOPAZ\"] = \"/lustre/storeB/project/copernicus/svalnav/Data_operational_ice_drift_forecasts/Pan_Arctic/TOPAZ4/\"\n",
    "paths[\"ECMWF\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/ECMWF/\"\n",
    "paths[\"AMSR2\"] = \"/lustre/storeB/project/copernicus/cosi/WP2/SIC/v0.1/\"\n",
    "paths[\"OSISAF_until_2020\"] = \"/lustre/storeB/project/copernicus/osisaf/data/reprocessed/ice/conc/v3p0/\"\n",
    "paths[\"OSISAF_from_2021\"] = \"/lustre/storeB/project/copernicus/osisaf/data/reprocessed/ice/conc-cont-reproc/v3p0/\"\n",
    "#\n",
    "proj = {}\n",
    "proj[\"TOPAZ\"] = \"+proj=stere +lon_0=-45. +lat_ts=90. +lat_0=90. +a=6378273. +b=6378273. +ellps=sphere\"\n",
    "proj[\"ECMWF\"] = \"+proj=latlon\"\n",
    "proj[\"OSISAF\"] = \"+proj=laea +lon_0=0 +datum=WGS84 +ellps=WGS84 +lat_0=90.0\"\n",
    "proj[\"AMSR2\"] = \"+ellps=WGS84 +lat_0=90 +lon_0=0 +no_defs=None +proj=laea +type=crs +units=m +x_0=0 +y_0=0\"\n",
    "#\n",
    "crs = {}\n",
    "for var in proj:\n",
    "    crs[var] = pyproj.CRS.from_proj4(proj[var])\n",
    "#\n",
    "variables = {}\n",
    "variables[\"LSM\"] = [\"LSM\"]\n",
    "variables[\"TOPAZ\"] = [\"fice\", \"hice\", \"temperature\"]\n",
    "variables[\"ECMWF\"] = [\"U10M\", \"V10M\", \"T2M\"]\n",
    "variables[\"OSISAF\"] = [\"ice_conc\", \"trend\"]  # trend is calculated from the last n days before the forecast start date\n",
    "variables[\"AMSR2\"] = [\"ice_conc\", \"trend\", \"total_standard_uncertainty\"] \n",
    "#\n",
    "Dates_AMSR2_missing_data = [\"20151204\", \"20160415\", \"20170928\", \"20171125\", \"20181216\", \"20210203\", \"20210620\", \"20210816\", \"20211102\", \"20220324\", \"20220413\", \"20220418\", \"20220729\", \"20221122\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3567e",
   "metadata": {},
   "source": [
    "task_date function\n",
    "\n",
    "    date_min: earliest forecast start date to process\n",
    "    date_max: latest forecast start date to process\n",
    "    task_ID: task ID when parallelizing (SGE_TASK_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "97132740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_date(date_min, date_max, task_ID):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    list_date = []\n",
    "    while current_date <= end_date:\n",
    "        list_date.append(current_date.strftime('%Y%m%d'))\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    date_task = list_date[task_ID - 1]\n",
    "    return(date_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70145483",
   "metadata": {},
   "source": [
    "ECMWF_time_steps_to_daily_time_steps function => Compute the daily mean of the variable for each days  \n",
    "    \n",
    "    time_ECMWF: time variable in ECMWF netCDF files\n",
    "    field: 2D array variable\n",
    "    ndays: number of days (lead time) to compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bcdf55a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ECMWF_time_steps_to_daily_time_steps(time_ECMWF, field, ndays):\n",
    "    lead_time = time_ECMWF - time_ECMWF[0]\n",
    "    ts_start = np.linspace(0 * 24, (ndays - 1) * 24, ndays)\n",
    "    ts_end = ts_start + 24\n",
    "    daily_field = np.full((ndays, field.shape[1], field.shape[2]), np.nan)\n",
    "    #\n",
    "    for ts in range(0, ndays):\n",
    "        lead_time_idx = np.squeeze(np.where(np.logical_and(lead_time >= ts_start[ts], lead_time < ts_end[ts])))\n",
    "        if ts == 3:\n",
    "            daily_field[ts,:,:] = (18 * np.nanmean(np.ma.squeeze(field[lead_time_idx[0:18],:,:]), axis = 0) + 6 * np.nanmean(np.ma.squeeze(field[lead_time_idx[18:20],:,:]), axis = 0)) / 24\n",
    "        else:\n",
    "            daily_field[ts,:,:] = np.nanmean(np.ma.squeeze(field[lead_time_idx,:,:]), axis = 0)\n",
    "    #\n",
    "    return(daily_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e9e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Padding function (make_padding)  \n",
    "\n",
    "    x and y must be vectors (can be latitude / longitude if the data are on a regular grid)  \n",
    "    field must be either a 2D array (y, x) or a 3D array (time, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "076491a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding(x, y, field):\n",
    "    dx = x[1] - x[0]\n",
    "    x_extent = np.pad(x, (1, 1), constant_values = np.nan)    \n",
    "    x_extent[0] = x_extent[1] - dx\n",
    "    x_extent[-1] = x_extent[-2] + dx\n",
    "    #\n",
    "    dy = y[1] - y[0]\n",
    "    y_extent = np.pad(y, (1, 1), constant_values = np.nan)\n",
    "    y_extent[0] = y_extent[1] - dy\n",
    "    y_extent[-1] = y_extent[-2] + dy\n",
    "    #\n",
    "    if field.ndim == 2:\n",
    "        field_extent = np.pad(field, (1,1), constant_values = np.nan)\n",
    "    elif field.ndim == 3:\n",
    "        time_dim = len(field[:,0,0])\n",
    "        field_extent = np.full((time_dim, len(y_extent), len(x_extent)), np.nan)\n",
    "        #\n",
    "        for t in range(0, time_dim):\n",
    "            field_extent[t,:,:] = np.pad(field[t,:,:], (1,1), constant_values = np.nan)\n",
    "    #\n",
    "    return(x_extent, y_extent, field_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070fb0d",
   "metadata": {},
   "source": [
    "Regridding functions (nearest_neighbor_indexes and nearest_neighbor_interp)  \n",
    "\n",
    "    xx_input and yy_input must be 2D arrays\n",
    "    x_output and y_output must be vectors  \n",
    "    field must be either a 2D array with dimensions (y, x) or a 3D array with dimensions (time, y, x) \n",
    "    invalid_values = fill value to replace by 0. Land is therefore considered as open ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d7a492d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_indexes(x_input, y_input, x_output, y_output):\n",
    "    x_input = np.expand_dims(x_input, axis = 1)\n",
    "    y_input = np.expand_dims(y_input, axis = 1)\n",
    "    x_output = np.expand_dims(x_output, axis = 1)\n",
    "    y_output = np.expand_dims(y_output, axis = 1)\n",
    "    #\n",
    "    coord_input = np.concatenate((x_input, y_input), axis = 1)\n",
    "    coord_output = np.concatenate((x_output, y_output), axis = 1)\n",
    "    #\n",
    "    tree = scipy.spatial.KDTree(coord_input)\n",
    "    dist, idx = tree.query(coord_output)\n",
    "    #\n",
    "    return(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "af838361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_interp(xx_input, yy_input, x_output, y_output, field, fill_value = None):\n",
    "    xx_input_flat = np.ndarray.flatten(xx_input)\n",
    "    yy_input_flat = np.ndarray.flatten(yy_input)\n",
    "    #\n",
    "    if fill_value is not None:\n",
    "        if field.ndim == 2:\n",
    "            idx_fill_value = np.ndarray.flatten(field) == fill_value\n",
    "        elif field.ndim == 3:\n",
    "            idx_fill_value = np.ndarray.flatten(field[0,:,:]) == fill_value\n",
    "        #\n",
    "        xx_input_flat = xx_input_flat[idx_fill_value == False]\n",
    "        yy_input_flat = yy_input_flat[idx_fill_value == False]\n",
    "    #\n",
    "    xx_output, yy_output = np.meshgrid(x_output, y_output)\n",
    "    xx_output_flat = np.ndarray.flatten(xx_output)\n",
    "    yy_output_flat = np.ndarray.flatten(yy_output)\n",
    "    #\n",
    "    idx = nearest_neighbor_indexes(xx_input_flat, yy_input_flat, xx_output_flat, yy_output_flat)\n",
    "    #\n",
    "    if field.ndim == 2:\n",
    "        field_flat = np.ndarray.flatten(field)\n",
    "        if fill_value is not None:\n",
    "            field_flat = field_flat[idx_fill_value == False]\n",
    "        #\n",
    "        field_interp = field_flat[idx]\n",
    "        field_regrid = np.reshape(field_interp, (len(y_output), len(x_output)), order = \"C\")\n",
    "    #    \n",
    "    elif field.ndim == 3:\n",
    "        time_dim = len(field[:,0,0])\n",
    "        field_regrid = np.full((time_dim, len(y_output), len(x_output)), np.nan)\n",
    "        #\n",
    "        for t in range(0, time_dim):\n",
    "            field_flat = np.ndarray.flatten(field[t,:,:])\n",
    "            if fill_value is not None:\n",
    "                field_flat = field_flat[idx_fill_value == False]\n",
    "            #\n",
    "            field_interp = field_flat[idx]\n",
    "            field_regrid[t,:,:] = np.reshape(field_interp, (len(y_output), len(x_output)), order = \"C\")\n",
    "    #\n",
    "    return(field_regrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db47417",
   "metadata": {},
   "source": [
    "rotate_wind function\n",
    "\n",
    "    x_wind, y_wind, lats, lons must be numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "11b38edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_wind(x_wind, y_wind, lats, lons, proj_str_from, proj_str_to):\n",
    "    if np.shape(x_wind) != np.shape(y_wind):\n",
    "        raise ValueError(f\"x_wind {np.shape(x_wind)} and y_wind {np.shape(y_wind)} arrays must be the same size\")\n",
    "    if len(lats.shape) != 1:\n",
    "        raise ValueError(f\"lats {np.shape(lats)} must be 1D\")\n",
    "    if np.shape(lats) != np.shape(lons):\n",
    "        raise ValueError(f\"lats {np.shape(lats)} and lats {np.shape(lons)} must be the same size\")\n",
    "    if len(np.shape(x_wind)) == 1:\n",
    "        if np.shape(x_wind) != np.shape(lats):\n",
    "            raise ValueError(f\"x_wind {len(x_wind)} and lats {len(lats)} arrays must be the same size\")\n",
    "    elif len(np.shape(x_wind)) == 2:\n",
    "        if x_wind.shape[1] != len(lats):\n",
    "            raise ValueError(f\"Second dimension of x_wind {x_wind.shape[1]} must equal number of lats {len(lats)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"x_wind {np.shape(x_wind)} must be 1D or 2D\")\n",
    "    #\n",
    "    proj_from = pyproj.Proj(proj_str_from)\n",
    "    proj_to = pyproj.Proj(proj_str_to)\n",
    "    transformer = pyproj.transformer.Transformer.from_proj(proj_from, proj_to)\n",
    "    #\n",
    "    orig_speed = np.sqrt(x_wind**2 + y_wind**2)\n",
    "    #\n",
    "    x0, y0 = proj_from(lons, lats)\n",
    "    if proj_from.name != \"longlat\":\n",
    "        x1 = x0 + x_wind\n",
    "        y1 = y0 + y_wind\n",
    "    else:\n",
    "        factor = 3600000.0\n",
    "        x1 = x0 + x_wind / factor / np.cos(lats * 3.14159265 / 180)\n",
    "        y1 = y0 + y_wind / factor\n",
    "    #\n",
    "    X0, Y0 = transformer.transform(x0, y0)\n",
    "    X1, Y1 = transformer.transform(x1, y1)\n",
    "    #\n",
    "    new_x_wind = X1 - X0\n",
    "    new_y_wind = Y1 - Y0\n",
    "    #\n",
    "    if proj_to.name == \"longlat\":\n",
    "        new_x_wind *= np.cos(lats * 3.14159265 / 180)\n",
    "    #\n",
    "    if proj_to.name == \"longlat\" or proj_from.name == \"longlat\":\n",
    "        curr_speed = np.sqrt(new_x_wind**2 + new_y_wind**2)\n",
    "        new_x_wind *= orig_speed / curr_speed\n",
    "        new_y_wind *= orig_speed / curr_speed\n",
    "    #\n",
    "    return(new_x_wind, new_y_wind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31087e6a",
   "metadata": {},
   "source": [
    "total_standard_uncertaintyread_netCDF functions  \n",
    "\n",
    "    filename: filename including the path\n",
    "    variables: list of variables (excluding time, x, y, lat, lon) to extract (list of strings)\n",
    "    paths: dictionary defined in the Constants section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0e57480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_netCDF(filename, variables, paths = paths):\n",
    "    Dataset = {}\n",
    "    nc = netCDF4.Dataset(filename, \"r\")\n",
    "    #\n",
    "    if (paths[\"LSM\"] in filename) == False:\n",
    "        Dataset[\"time\"] = nc.variables[\"time\"][:]\n",
    "    #\n",
    "    if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "        xmin = 65\n",
    "        xmax = 609\n",
    "        ymin = 70\n",
    "        ymax = 614\n",
    "        Dataset[\"x\"] = nc.variables[\"x\"][xmin:xmax] * 100 * 1000\n",
    "        Dataset[\"y\"] = nc.variables[\"y\"][ymin:ymax] * 100 * 1000\n",
    "        Dataset[\"lat\"] = nc.variables[\"latitude\"][ymin:ymax, xmin:xmax] \n",
    "        Dataset[\"lon\"] = nc.variables[\"longitude\"][ymin:ymax, xmin:xmax]\n",
    "    #\n",
    "    elif paths[\"ECMWF\"] in filename:\n",
    "        Dataset[\"lat\"] = nc.variables[\"lat\"][:]\n",
    "        Dataset[\"lon\"] = nc.variables[\"lon\"][:]\n",
    "    #\n",
    "    elif (paths[\"OSISAF_until_2020\"] in filename) or (paths[\"OSISAF_from_2021\"] in filename) or (paths[\"AMSR2\"] in filename):\n",
    "        Dataset[\"x\"] = nc.variables[\"xc\"][:] * 1000\n",
    "        Dataset[\"y\"] = nc.variables[\"yc\"][:] * 1000\n",
    "        Dataset[\"lat\"] = nc.variables[\"lat\"][:,:] \n",
    "        Dataset[\"lon\"] = nc.variables[\"lon\"][:,:]\n",
    "    #\n",
    "    for var in variables:\n",
    "        vardim = nc.variables[var].ndim\n",
    "        if vardim == 1:\n",
    "            Dataset[var] = nc.variables[var][:]\n",
    "        elif vardim == 2:\n",
    "            if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "                Dataset[var] = nc.variables[var][ymin:ymax, xmin:xmax]\n",
    "            else:\n",
    "                Dataset[var] = nc.variables[var][:,:]\n",
    "        elif vardim == 3:\n",
    "            if (paths[\"LSM\"] in filename) or (paths[\"TOPAZ\"] in filename):\n",
    "                if var == \"fice\":\n",
    "                    Dataset[var] = nc.variables[var][:, ymin:ymax, xmin:xmax] * 100\n",
    "                else:\n",
    "                    Dataset[var] = nc.variables[var][:, ymin:ymax, xmin:xmax]\n",
    "            else:\n",
    "                Dataset[var] = nc.variables[var][:,:,:]\n",
    "                if paths[\"AMSR2\"] in filename:\n",
    "                    filename_date = os.path.basename(filename)[13:21]\n",
    "                    if filename_date in Dates_AMSR2_missing_data:\n",
    "                        Dataset[var] = np.full((1, 2160, 2160), np.nan)\n",
    "                    else:\n",
    "                        if (\"ice_conc\" in var) or (\"total_standard_uncertainty\" in var):\n",
    "                            idx_invalid_values = Dataset[var] < 0\n",
    "                            Dataset[var][idx_invalid_values == True] = -32767\n",
    "        elif vardim == 4:\n",
    "            if paths[\"TOPAZ\"] in filename:\n",
    "                Dataset[var] = nc.variables[var][:,0,ymin:ymax, xmin:xmax]\n",
    "        else:\n",
    "            print(\"ERROR. Number of dimensions higher than 3.\")\n",
    "    nc.close()\n",
    "    #\n",
    "    return(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0002e5",
   "metadata": {},
   "source": [
    "extract_TOPAZ_data function\n",
    "\n",
    "    date_task: forecast start date of the TOPAZ forecasts\n",
    "    ndays: maximum lead time in days\n",
    "    variables: list of variables to extract (variables[\"TOPAZ\"]) \n",
    "    paths = path where the TOPAZ data are stored (paths[\"TOPAZ\"]). Defined in the Constant section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dc54bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_TOPAZ_data(date_task, ndays, variables = variables[\"TOPAZ\"], paths = paths):\n",
    "    Data_output = {}\n",
    "    filename_TOPAZ = paths[\"TOPAZ\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\" + \"TOPAZ4_daily_\" + date_task + \".nc\"\n",
    "    TOPAZ = read_netCDF(filename_TOPAZ, variables)\n",
    "    #\n",
    "    for var in [\"time\", \"x\", \"y\", \"lat\", \"lon\"]:\n",
    "        Data_output[var] = TOPAZ[var]\n",
    "    #\n",
    "    xx, yy = np.meshgrid(Data_output[\"x\"], Data_output[\"y\"])\n",
    "    xx_flat = np.ndarray.flatten(xx)\n",
    "    yy_flat = np.ndarray.flatten(yy)\n",
    "    #\n",
    "    for var in variables:\n",
    "        if (var == \"fice\") or (var == \"hice\"):\n",
    "            idx_assign = np.logical_or(np.isnan(TOPAZ[var]) == True, TOPAZ[var] == -32767)\n",
    "            Data_output[var] = np.copy(TOPAZ[var])\n",
    "            Data_output[var][idx_assign == True] = 0\n",
    "        elif var == \"temperature\":\n",
    "            Data_output[var + \"_cum\"] = np.full(np.shape(TOPAZ[var]), np.nan)\n",
    "            for lt in range(0, ndays):\n",
    "                Data_output[var + \"_cum\"][lt,:,:] = np.nanmean(TOPAZ[var][0:lt+1,:,:], axis = 0)\n",
    "            #\n",
    "            idx_assign = np.logical_or(np.isnan(Data_output[var + \"_cum\"]) == True, Data_output[var + \"_cum\"] == -32767)\n",
    "            Data_output[var + \"_cum\"][idx_assign == True] =  0  \n",
    "    #\n",
    "    return(Data_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c3d5c",
   "metadata": {},
   "source": [
    "extract_ECMWF_data function   \n",
    "\n",
    "    filename: filename (including path) containing ECMWF data \n",
    "    ndays: maximum lead time in days\n",
    "    TOPAZ: TOPAZ dataset (dictionary)   \n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract (variables[\"ECMWF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "865df8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_ECMWF_data(filename, ndays, TOPAZ, proj = proj, variables = variables[\"ECMWF\"], crs = crs):\n",
    "    ECMWF = read_netCDF(filename, variables)\n",
    "    Data_TOPAZgrid = {}\n",
    "    transform_ECMWF_to_TOPAZ = pyproj.Transformer.from_crs(crs[\"ECMWF\"], crs[\"TOPAZ\"], always_xy = True)\n",
    "    lons, lats = np.meshgrid(ECMWF[\"lon\"], ECMWF[\"lat\"])\n",
    "    xx_ECMWF_TOPAZproj, yy_ECMWF_TOPAZproj = transform_ECMWF_to_TOPAZ.transform(lons, lats)\n",
    "    #\n",
    "    Data_ECMWFgrid = {}\n",
    "    for var in variables:\n",
    "        Data_ECMWFgrid[var] = ECMWF_time_steps_to_daily_time_steps(ECMWF[\"time\"], ECMWF[var], ndays)\n",
    "    #\n",
    "    if (\"U10M\" in Data_ECMWFgrid) and (\"V10M\" in Data_ECMWFgrid):\n",
    "        x_wind = np.full((len(TOPAZ[\"time\"]), len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        y_wind = np.full((len(TOPAZ[\"time\"]), len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        #\n",
    "        for ts in range(0, len(TOPAZ[\"time\"])):\n",
    "            x_wind_rot, y_wind_rot = rotate_wind(np.ndarray.flatten(Data_ECMWFgrid[\"U10M\"][ts,:,:]), \n",
    "                                                 np.ndarray.flatten(Data_ECMWFgrid[\"V10M\"][ts,:,:]),\n",
    "                                                 np.ndarray.flatten(lats), \n",
    "                                                 np.ndarray.flatten(lons), \n",
    "                                                 proj[\"ECMWF\"], \n",
    "                                                 proj[\"TOPAZ\"]\n",
    "                                                )\n",
    "            #\n",
    "            x_wind[ts,:,:] = np.reshape(x_wind_rot, (len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), order = \"C\")\n",
    "            y_wind[ts,:,:] = np.reshape(y_wind_rot, (len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), order = \"C\")            \n",
    "            #\n",
    "        Data_ECMWFgrid[\"wind_x\"] = np.copy(x_wind)\n",
    "        Data_ECMWFgrid[\"wind_y\"] = np.copy(y_wind)\n",
    "        Data_ECMWFgrid.pop(\"U10M\")\n",
    "        Data_ECMWFgrid.pop(\"V10M\")\n",
    "    #\n",
    "    Cum_data_ECMWFgrid = {}\n",
    "    for var in Data_ECMWFgrid:\n",
    "        var_cum = np.full((len(TOPAZ[\"time\"]), len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        for ts in range(0, len(TOPAZ[\"time\"])):\n",
    "            var_cum[ts,:,:] = np.nanmean(Data_ECMWFgrid[var][0:ts+1,:,:], axis = 0)\n",
    "        #\n",
    "        Cum_data_ECMWFgrid[var + \"_cum\"] = np.copy(var_cum)\n",
    "        Cum_data_ECMWFgrid[var + \"_cum\"][np.isnan(var_cum) == True] = -32767\n",
    "    #\n",
    "    for var in Cum_data_ECMWFgrid:\n",
    "        Data_TOPAZgrid[var] = nearest_neighbor_interp(xx_ECMWF_TOPAZproj, yy_ECMWF_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], Cum_data_ECMWFgrid[var], fill_value = -32767)\n",
    "    #\n",
    "    return(Data_TOPAZgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a59c79",
   "metadata": {},
   "source": [
    "extract_SIC_obs_predictors function\n",
    "    \n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    trend_period: Number of days to take into account for calculating the trend\n",
    "    TOPAZ: TOPAZ dataset (dictionary)   \n",
    "    LSM: TOPAZ land sea mask (dictionary)\n",
    "    dataset: \"AMSR2\" or \"OSISAF\"    dataset: \"AMSR2\" or \"OSISAF\"\n",
    "    proj: dictionary of proj4 strings\n",
    "    crs: crs defined in \"Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "547fd833-f20e-4967-91a3-2e1591662082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_SIC_obs_predictors(date_task, trend_period, TOPAZ, LSM, dataset, proj = proj, crs = crs):\n",
    "    Data_TOPAZgrid = {}\n",
    "    #\n",
    "    LSM_extend = np.expand_dims(LSM[\"LSM\"], axis = 0)\n",
    "    transform_SIC_to_TOPAZ = pyproj.Transformer.from_crs(crs[dataset], crs[\"TOPAZ\"], always_xy = True)\n",
    "    #\n",
    "    first_date = datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = trend_period)\n",
    "    last_date = datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = 1)\n",
    "    current_date = first_date\n",
    "    time_vect = []\n",
    "    #\n",
    "    for id in range(0, trend_period):\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        if dataset == \"AMSR2\":\n",
    "            date_day_after_str = (datetime.datetime.strptime(date_str, \"%Y%m%d\") + datetime.timedelta(days = 1)).strftime(\"%Y%m%d\")\n",
    "            filename_SIC = paths[\"AMSR2\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"sic_cosi-5km_\" + date_str + \"0000-\" + date_day_after_str + \"0000.nc\"\n",
    "        elif dataset == \"OSISAF\":\n",
    "            if int(date_str[0:4]) < 2021:\n",
    "                filename_SIC = paths[\"OSISAF_until_2020\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_cdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "            else:\n",
    "                filename_SIC = paths[\"OSISAF_from_2021\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_icdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "        #\n",
    "        if os.path.isfile(filename_SIC):\n",
    "            if (dataset == \"AMSR2\") and (date_str in Dates_AMSR2_missing_data):\n",
    "                pass\n",
    "            else:\n",
    "                time_vect.append(id)\n",
    "                SIC_data = read_netCDF(filename_SIC, variables = [\"ice_conc\"])\n",
    "                xx_SIC, yy_SIC = np.meshgrid(SIC_data[\"x\"], SIC_data[\"y\"])\n",
    "                xx_SIC_TOPAZproj, yy_SIC_TOPAZproj = transform_SIC_to_TOPAZ.transform(xx_SIC, yy_SIC)\n",
    "                SIC_TOPAZgrid = nearest_neighbor_interp(xx_SIC_TOPAZproj, yy_SIC_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], SIC_data[\"ice_conc\"], fill_value = -32767)\n",
    "                SIC_TOPAZgrid[LSM_extend == 0] = 0\n",
    "                #\n",
    "                if \"SIC_all\" in locals():\n",
    "                    SIC_all = np.concatenate((SIC_all, SIC_TOPAZgrid), axis = 0)\n",
    "                else:\n",
    "                    SIC_all = np.copy(SIC_TOPAZgrid)\n",
    "                #\n",
    "                if current_date == last_date:\n",
    "                    Data_TOPAZgrid[\"ice_conc\"] = np.squeeze(SIC_TOPAZgrid)\n",
    "                #\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    #\n",
    "    if len(time_vect) >= 2:\n",
    "        trend = np.zeros((SIC_all.shape[1], SIC_all.shape[2]))\n",
    "        x = np.array(time_vect).reshape((-1,1))\n",
    "        for i in range(0, SIC_all.shape[1]):\n",
    "            for j in range(0, SIC_all.shape[2]):\n",
    "                y = SIC_all[:,i,j]\n",
    "                if np.all(y >= 0):\n",
    "                    model = sklearn.linear_model.LinearRegression().fit(x, y)\n",
    "                    trend[i,j] = model.coef_[0]\n",
    "        Data_TOPAZgrid[\"trend\"] = np.squeeze(trend)\n",
    "    #\n",
    "    return(Data_TOPAZgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f4f64",
   "metadata": {},
   "source": [
    "extract_targets \n",
    "\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    ndays: Number of days to take into account for calculating the trend\n",
    "    TOPAZ: TOPAZ dataset (dictionary)\n",
    "    LSM: TOPAZ land sea maskLSM: TOPAZ land sea mask\n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract (variables[\"OSISAF\"])\n",
    "    crs: crs defined in \"Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "994c3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(date_task, ndays, TOPAZ, LSM, dataset, proj = proj, crs = crs, paths = paths, variables = variables):\n",
    "    Data_TOPAZgrid = {}\n",
    "    #\n",
    "    transform_SIC_to_TOPAZ = pyproj.Transformer.from_crs(crs[dataset], crs[\"TOPAZ\"], always_xy = True)\n",
    "    #\n",
    "    for lt in range(0, ndays):\n",
    "        date_str = (datetime.datetime.strptime(date_task, \"%Y%m%d\") + datetime.timedelta(days = lt)).strftime(\"%Y%m%d\")\n",
    "        if dataset == \"AMSR2\":\n",
    "            date_day_after_str = (datetime.datetime.strptime(date_task, \"%Y%m%d\") + datetime.timedelta(days = lt + 1)).strftime(\"%Y%m%d%H%M\")\n",
    "            filename_SIC = paths[\"AMSR2\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"sic_cosi-5km_\" + date_str + \"0000-\" + date_day_after_str + \".nc\"\n",
    "        elif dataset == \"OSISAF\":\n",
    "            if int(date_str[0:4]) < 2021:\n",
    "                filename_SIC = paths[\"OSISAF_until_2020\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_cdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "            else:\n",
    "                filename_SIC = paths[\"OSISAF_from_2021\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"ice_conc_nh_ease2-250_icdr-v3p0_\" + date_str + \"1200.nc\"\n",
    "        #\n",
    "        SIC_obs = read_netCDF(filename_SIC, variables = [\"ice_conc\", \"total_standard_uncertainty\"])\n",
    "        #\n",
    "        if lt == 0:\n",
    "            xx_SIC_obs, yy_SIC_obs = np.meshgrid(SIC_obs[\"x\"], SIC_obs[\"y\"])\n",
    "            xx_SIC_TOPAZproj, yy_SIC_TOPAZproj = transform_SIC_to_TOPAZ.transform(xx_SIC_obs, yy_SIC_obs)\n",
    "        #\n",
    "        SIC_TOPAZgrid = nearest_neighbor_interp(xx_SIC_TOPAZproj, yy_SIC_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], SIC_obs[\"ice_conc\"], fill_value = -32767)\n",
    "        total_uncertainty_TOPAZgrid = nearest_neighbor_interp(xx_SIC_TOPAZproj, yy_SIC_TOPAZproj, TOPAZ[\"x\"], TOPAZ[\"y\"], SIC_obs[\"total_standard_uncertainty\"], fill_value = -32767)\n",
    "        #\n",
    "        ls_mask = np.expand_dims(LSM[\"LSM\"], axis = 0)\n",
    "        SIC_TOPAZgrid[ls_mask == 0] = 0\n",
    "        total_uncertainty_TOPAZgrid[ls_mask == 0] = 0\n",
    "        #\n",
    "        if lt == 0:\n",
    "            Data_TOPAZgrid[\"SIC\"] = np.copy(SIC_TOPAZgrid)\n",
    "            Data_TOPAZgrid[\"SIC_total_standard_uncertainty\"] = np.copy(total_uncertainty_TOPAZgrid)\n",
    "        else:\n",
    "            Data_TOPAZgrid[\"SIC\"] = np.concatenate((Data_TOPAZgrid[\"SIC\"], SIC_TOPAZgrid), axis = 0)\n",
    "            Data_TOPAZgrid[\"SIC_total_standard_uncertainty\"] = np.concatenate((Data_TOPAZgrid[\"SIC_total_standard_uncertainty\"], total_uncertainty_TOPAZgrid), axis = 0)\n",
    "    #\n",
    "    Data_TOPAZgrid[\"SIE_10\"] = np.zeros(np.shape(Data_TOPAZgrid[\"SIC\"]))\n",
    "    Data_TOPAZgrid[\"SIE_20\"] = np.zeros(np.shape(Data_TOPAZgrid[\"SIC\"]))\n",
    "    Data_TOPAZgrid[\"SIE_10\"][Data_TOPAZgrid[\"SIC\"] >= 10] = 1\n",
    "    Data_TOPAZgrid[\"SIE_20\"][Data_TOPAZgrid[\"SIC\"] >= 20] = 1\n",
    "    #\n",
    "    Data_TOPAZgrid[\"TOPAZ_error\"] = TOPAZ[\"fice\"] - Data_TOPAZgrid[\"SIC\"]\n",
    "    #\n",
    "    return(Data_TOPAZgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2d51c",
   "metadata": {},
   "source": [
    "write_netCDF function\n",
    "\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    Datasets: Dictionary containing all variables that we want to extract\n",
    "    paths: paths defined in the Constants section\n",
    "    trend_period: Number of days to take into account for calculating the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "637ca072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_netCDF(date_task, Datasets, paths, trend_period):\n",
    "    Outputs = vars()\n",
    "    #\n",
    "    path_output = paths[\"output\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\"\n",
    "    if os.path.exists(path_output) == False:\n",
    "        os.system(\"mkdir -p \" + path_output)    \n",
    "    output_filename = path_output + \"Dataset_\" + date_task + \".nc\"\n",
    "    if os.path.isfile(output_filename):\n",
    "        os.system(\"rm \" + output_filename)\n",
    "    output_netcdf = netCDF4.Dataset(output_filename, 'w', format = 'NETCDF4')\n",
    "    #\n",
    "    dimensions = [\"time\", \"x\", \"y\"]\n",
    "    for di in dimensions:\n",
    "        Outputs[di] = output_netcdf.createDimension(di, len(Datasets[\"TOPAZ\"][di]))\n",
    "    #\n",
    "    dim_variables = dimensions + [\"lat\", \"lon\"]\n",
    "    for dv in dim_variables:\n",
    "        if Datasets[\"TOPAZ\"][dv].ndim == 1:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (dv))\n",
    "            Outputs[dv][:] = Datasets[\"TOPAZ\"][dv]\n",
    "            if dv == \"time\":\n",
    "                Outputs[dv].standard_name = \"forecast time\"\n",
    "                Outputs[dv].units = \"hours since 1950-1-1T00:00:00Z\"\n",
    "            elif dv == \"x\" or dv == \"y\":\n",
    "                Outputs[dv].standard_name = \"projection_\" + dv + \"_coordinate\"\n",
    "                Outputs[dv].units = \"m\"\n",
    "        elif Datasets[\"TOPAZ\"][dv].ndim == 2:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (\"y\", \"x\"))\n",
    "            Outputs[dv][:,:] = Datasets[\"TOPAZ\"][dv]\n",
    "            if dv == \"lat\":\n",
    "                Outputs[dv].standard_name = \"latitude\"\n",
    "            elif dv == \"lon\":\n",
    "                Outputs[dv].standard_name = \"longitude\"\n",
    "            Outputs[dv].units = \"degrees\"\n",
    "    #\n",
    "    SIC_variables = [\"ice_conc\", \"fice\", \"SIC\"]\n",
    "    for ds in Datasets:\n",
    "        for var in Datasets[ds]:\n",
    "            if (var in dim_variables) == False:\n",
    "                if var == \"LSM\":\n",
    "                    var_name = \"LSM\"\n",
    "                elif var in SIC_variables:\n",
    "                    var_name = ds + \"_SIC\"\n",
    "                else:\n",
    "                    var_name = ds + \"_\" + var\n",
    "                #\n",
    "                if Datasets[ds][var].ndim == 2:\n",
    "                    Outputs[var_name] = output_netcdf.createVariable(var_name, \"d\", (\"y\", \"x\"))\n",
    "                    Outputs[var_name][:,:] = np.round(Datasets[ds][var], 3)\n",
    "                elif Datasets[ds][var].ndim == 3:\n",
    "                    Outputs[var_name] = output_netcdf.createVariable(var_name, \"d\", (\"time\", \"y\", \"x\"))\n",
    "                    Outputs[var_name][:,:,:] = np.round(Datasets[ds][var], 3)\n",
    "                #\n",
    "                if var in SIC_variables:\n",
    "                    if ds == \"TARGET_AMSR2\":\n",
    "                        Outputs[var_name].standard_name = \"AMSR2 sea ice concentration\"\n",
    "                    elif ds == \"TARGET_OSISAF\":\n",
    "                        Outputs[var_name].standard_name = \"OSISAF sea ice concentration\"\n",
    "                    elif ds == \"SICobs_AMSR2\":\n",
    "                        Outputs[var_name].standard_name = \"Sea ice concentration from AMSR2 during the day preceding the forecast start date\"\n",
    "                    elif ds == \"SICobs_OSISAF\":\n",
    "                        Outputs[var_name].standard_name = \"Sea ice concentration from OSISAF during the day preceding the forecast start date\"\n",
    "                    else:\n",
    "                        Outputs[var_name].standard_name = ds + \" sea ice concentration\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                if \"SIE_\" in var:\n",
    "                    Outputs[var_name].standard_name = \"Sea ice extent with a sea ice concentration higher than \" + var[-2:len(var)] + \" %\"\n",
    "                    Outputs[var_name].units = \"1 if sea ice concentration higher than \" + var[-2:len(var)] + \" %, 0 otherwise\"\n",
    "                elif var == \"SIC_total_standard_uncertainty\":\n",
    "                    Outputs[var_name].standard_name = \"Total uncertainty (one standard deviation) of concentration of sea ice\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                elif var == \"trend\":\n",
    "                    Outputs[var_name].standard_name = \"Sea ice concentration trend over the \" + str(SIC_trend_period) + \" days preceding the forecast start date\"\n",
    "                    Outputs[var_name].units = \"% / day\"\n",
    "                elif var == \"TOPAZ_error\":\n",
    "                    Outputs[var_name].standard_name = \"TOPAZ error in sea ice concentration (TOPAZ - AMSR2 observations)\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                elif ds + \"_\" + var == \"TOPAZ_hice\":\n",
    "                    Outputs[var_name].standard_name = \"TOPAZ sea ice thickness\"\n",
    "                    Outputs[var_name].units = \"m\"\n",
    "                elif ds + \"_\" + var == \"TOPAZ_temperature_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean sea surface temperature since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"degree Celsius\"\n",
    "                elif var == \"LSM\":\n",
    "                    Outputs[var_name].standard_name = \"Land sea mask\"\n",
    "                    Outputs[var_name].units = \"1: ocean, 0: land\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_T2M_cum\":\n",
    "                    Outputs[var_name].standard_name = \"ECMWF 2 metre temperature\"\n",
    "                    Outputs[var_name].units = \"K\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_wind_x_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean ECMWF wind in the x direction since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"m/s\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_wind_y_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean ECMWF wind in the y direction since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"m/s\"\n",
    "    output_netcdf.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b575d",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7b4c756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_task 20210618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2174496/210722228.py:21: RuntimeWarning: Mean of empty slice\n",
      "  Data_output[var + \"_cum\"][lt,:,:] = np.nanmean(TOPAZ[var][0:lt+1,:,:], axis = 0)\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_2174496/2526028044.py:37: RuntimeWarning: Mean of empty slice\n",
      "  var_cum[ts,:,:] = np.nanmean(Data_ECMWFgrid[var][0:ts+1,:,:], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.13110613822937\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#\n",
    "date_task = task_date(date_min, date_max, task_ID = SGE_TASK_ID)\n",
    "previous_day = (datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = 1)).strftime(\"%Y%m%d\")\n",
    "print(\"date_task\", date_task)\n",
    "if previous_day in Dates_AMSR2_missing_data:\n",
    "    print(\"Missing AMSR2 observations during the day preceding the forecast start date\")\n",
    "else:\n",
    "    #\n",
    "    filename_LSM = paths[\"LSM\"] + \"TOPAZ4_land_sea_mask.nc\"            \n",
    "    filename_ECMWF = paths[\"ECMWF\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/ECMWF_operational_forecasts_T2m_10mwind_\" + date_task + \"_NH.nc\"\n",
    "    #\n",
    "    Datasets = {}\n",
    "    #\n",
    "    Datasets[\"LSM\"] = read_netCDF(filename = filename_LSM, \n",
    "                                  variables = variables[\"LSM\"])   # 1 ocean / 0 land\n",
    "    #\n",
    "    Datasets[\"TOPAZ\"] = extract_TOPAZ_data(date_task = date_task, \n",
    "                                           ndays = lead_time_max, \n",
    "                                           variables = variables[\"TOPAZ\"], \n",
    "                                           paths = paths)\n",
    "    #\n",
    "    Datasets[\"ECMWF\"] = extract_ECMWF_data(filename = filename_ECMWF, \n",
    "                                           ndays = lead_time_max, \n",
    "                                           TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                           variables = variables[\"ECMWF\"], \n",
    "                                           crs = crs)\n",
    "    #\n",
    "    try:\n",
    "        Datasets[\"SICobs_OSISAF\"] = extract_SIC_obs_predictors(date_task = date_task, \n",
    "                                                        trend_period = SIC_trend_period, \n",
    "                                                        TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                                        LSM = Datasets[\"LSM\"],\n",
    "                                                        dataset = \"OSISAF\", \n",
    "                                                        proj = proj, \n",
    "                                                        crs = crs)\n",
    "    except:\n",
    "        pass\n",
    "    #\n",
    "    Datasets[\"SICobs_AMSR2\"] = extract_SIC_obs_predictors(date_task = date_task, \n",
    "                                                          trend_period = SIC_trend_period, \n",
    "                                                          TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                                          LSM = Datasets[\"LSM\"],\n",
    "                                                          dataset = \"AMSR2\", \n",
    "                                                          proj = proj, \n",
    "                                                          crs = crs)\n",
    "    #\n",
    "    Datasets[\"TARGET_AMSR2\"] = extract_targets(date_task = date_task, \n",
    "                                               ndays = lead_time_max, \n",
    "                                               TOPAZ = Datasets[\"TOPAZ\"], \n",
    "                                               LSM = Datasets[\"LSM\"], \n",
    "                                               dataset = \"AMSR2\",\n",
    "                                               proj = proj, \n",
    "                                               crs = crs, \n",
    "                                               paths = paths)\n",
    "    #\n",
    "    write_netCDF(date_task = date_task, \n",
    "                 Datasets = Datasets, \n",
    "                 paths = paths, \n",
    "                 trend_period = SIC_trend_period)\n",
    "    #\n",
    "    t1 = time.time()\n",
    "    print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ffe4e-9172-447c-bc3e-63f8bc7ccff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27811b45-250e-4f5a-8e68-b527cbb1b5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
